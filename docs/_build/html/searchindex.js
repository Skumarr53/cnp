Search.setIndex({"docnames": ["index", "source/common_utils", "source/data_access", "source/data_processing", "source/embedding", "source/preprocessing", "source/text_processing", "source/utils"], "filenames": ["index.rst", "source/common_utils.rst", "source/data_access.rst", "source/data_processing.rst", "source/embedding.rst", "source/preprocessing.rst", "source/text_processing.rst", "source/utils.rst"], "titles": ["Welcome to Centralized NLP Package\u2019s documentation!", "Common Utils", "Data Access", "Data Processing", "Embedding", "Preprocessing", "Text processing", "Utilities"], "terms": {"data": [0, 1, 6, 7], "access": 0, "execute_snowflake_query_spark": [0, 2], "execute_truncate_or_merge_queri": [0, 2], "get_snowflake_connection_opt": [0, 2], "retrieve_snowflake_private_kei": [0, 2], "singleton": [0, 2], "with_spark_sess": [0, 2], "write_dataframe_to_snowflak": [0, 2], "dask_compute_with_progress": [0, 2], "initialize_dask_cli": [0, 2], "embed": 0, "average_token_embed": [0, 4], "save_word2vec_model": [0, 4], "train_word2vec_model": [0, 4], "preprocess": 0, "clean_text": [0, 5], "initialize_spaci": [0, 5], "preprocess_text": [0, 5], "preprocess_text_list": [0, 5], "remove_unwanted_phrases_and_valid": [0, 5], "tokenize_and_lemmatize_text": [0, 5], "tokenize_matched_word": [0, 5], "text": [0, 1, 5], "process": [0, 2, 5, 7], "combine_sentiment_scor": [0, 6], "expand_contract": [0, 6], "generate_ngram": [0, 6], "load_set_from_txt": [0, 6], "load_syllable_count": [0, 6], "tokenize_text": [0, 6], "validate_and_format_text": [0, 6], "calculate_polarity_scor": [0, 6], "calculate_sentence_scor": [0, 6], "check_neg": [0, 6], "fog_analysis_per_sect": [0, 6], "fog_analysis_per_sent": [0, 6], "generate_match_count": [0, 6], "generate_sentence_relevance_scor": [0, 6], "generate_topic_statist": [0, 6], "get_match_set": [0, 6], "is_complex": [0, 6], "load_word_set": [0, 6], "match_count": [0, 6], "merge_count": [0, 6], "netscor": [0, 6], "polarity_score_per_sect": [0, 6], "polarity_score_per_sent": [0, 6], "tone_count_with_negation_check": [0, 6], "tone_count_with_negation_check_per_sent": [0, 6], "util": 0, "df_apply_transform": [0, 3, 7], "df_remove_rows_with_keyword": [0, 3, 7], "format_d": [0, 1, 7], "format_string_templ": [0, 1, 7], "get_date_rang": [0, 1, 7], "load_fil": [0, 7], "query_constructor": [0, 1, 7], "common": 0, "load_content_from_txt": [0, 1], "index": [0, 6], "modul": 0, "search": 0, "page": 0, "centralized_nlp_packag": [1, 2, 3, 4, 5, 6, 7], "common_util": 1, "date_util": 1, "date": [1, 7], "sourc": [1, 2, 3, 4, 5, 6, 7], "format": [1, 2, 4, 6, 7], "datetim": [1, 7], "object": [1, 2, 6, 7], "string": [1, 3, 6, 7], "yyyi": [1, 7], "mm": [1, 7], "dd": [1, 2, 3, 7], "paramet": [1, 2, 3, 4, 5, 6, 7], "The": [1, 2, 3, 4, 5, 6, 7], "return": [1, 2, 3, 4, 5, 6, 7], "type": [1, 2, 3, 4, 5, 6, 7], "str": [1, 2, 3, 4, 5, 6, 7], "exampl": [1, 2, 3, 4, 5, 6, 7], "from": [1, 2, 4, 6, 7], "import": [1, 2, 4, 6, 7], "2023": [1, 7], "9": [1, 7], "15": [1, 7], "09": [1, 7], "years_back": [1, 7], "0": [1, 2, 3, 4, 5, 6, 7], "months_back": [1, 7], "calcul": [1, 6, 7], "rang": [1, 7], "base": [1, 3, 6, 7], "current": [1, 6, 7], "minu": [1, 7], "specifi": [1, 2, 3, 4, 6, 7], "year": [1, 7], "month": [1, 7], "int": [1, 2, 5, 6, 7], "option": [1, 2, 4, 5, 6, 7], "number": [1, 2, 5, 6, 7], "go": [1, 5, 6, 7], "back": [1, 7], "default": [1, 2, 4, 6, 7], "A": [1, 2, 3, 4, 6, 7], "tupl": [1, 3, 5, 6, 7], "contain": [1, 2, 3, 6, 7], "start": [1, 7], "end": [1, 7], "start_dat": [1, 7], "end_dat": [1, 7], "1": [1, 2, 3, 4, 6, 7], "2": [1, 2, 3, 5, 6, 7], "exmapl": 1, "i": [1, 2, 3, 5, 6, 7], "10": [1, 4, 6, 7], "01": [1, 7], "print": [1, 2, 3, 4, 5, 6, 7], "2022": [1, 7], "08": [1, 7], "file_util": 1, "file_path": [1, 6, 7], "read": [1, 6], "entir": 1, "content": [1, 5, 6, 7], "file": [1, 4, 6, 7], "given": [1, 3, 6, 7], "path": [1, 4, 6, 7], "rais": [1, 2, 3, 6, 7], "filesnotloadedexcept": [1, 6, 7], "If": [1, 2, 3, 6, 7], "found": [1, 3, 6, 7], "sampl": [1, 5], "txt": [1, 6], "thi": [1, 2, 4, 5, 6], "string_util": 1, "templat": [1, 7], "kwarg": [1, 4, 7], "construct": [1, 2, 7], "replac": [1, 7], "placehold": [1, 7], "provid": [1, 4, 6, 7], "keyword": [1, 3, 4, 7], "argument": [1, 4, 7], "form": [1, 7], "kei": [1, 2, 6, 7], "variabl": [1, 7], "valueerror": [1, 3, 7], "ani": [1, 2, 3, 4, 6, 7], "doe": [1, 7], "have": [1, 7], "correspond": [1, 7], "hello": [1, 4, 5, 6, 7], "name": [1, 2, 3, 6, 7], "todai": [1, 7], "dai": [1, 7], "alic": [1, 7], "mondai": [1, 7], "query_input": [1, 7], "queri": [1, 2, 7], "load": [1, 6, 7], "us": [1, 3, 4, 6, 7], "union": [1, 3, 5, 6, 7], "o": [1, 7], "pathlik": [1, 7], "itself": [1, 7], "substitut": [1, 7], "do": [1, 6, 7], "match": [1, 5, 6, 7], "exist": [1, 2, 4, 7], "select": [1, 2, 7], "user": [1, 2, 7], "where": [1, 3, 6, 7], "signup_d": [1, 7], "AND": [1, 7], "12": [1, 6, 7], "31": [1, 7], "data_access": 2, "snowflake_util": 2, "execut": 2, "sql": [2, 7], "snowflak": 2, "result": [2, 6], "spark": 2, "datafram": [2, 3, 6, 7], "except": [2, 3, 7], "an": [2, 4, 6, 7], "error": [2, 6], "df": [2, 3, 6, 7], "my_tabl": 2, "truncat": 2, "merg": [2, 6], "confirm": 2, "messag": 2, "indic": [2, 6], "complet": 2, "oper": 2, "tabl": 2, "dictionari": [2, 6], "connect": 2, "includ": [2, 6], "account": [2, 6], "url": 2, "credenti": 2, "privat": 2, "databas": 2, "schema": 2, "timezon": 2, "role": 2, "dict": [2, 6], "snowflake_opt": 2, "retriev": 2, "azur": 2, "vault": 2, "akv": 2, "function": [2, 3, 4, 7], "fetch": 2, "encrypt": 2, "password": 2, "decrypt": 2, "authent": 2, "pem": 2, "suitabl": 2, "private_kei": 2, "cl": 2, "decor": 2, "make": 2, "class": 2, "ensur": [2, 4, 6], "onli": [2, 6], "one": 2, "instanc": 2, "func": [2, 3, 7], "session": 2, "initi": [2, 4, 5, 6], "befor": 2, "table_nam": 2, "mode": 2, "append": 2, "write": 2, "target": 2, "behavior": 2, "alreadi": 2, "ar": [2, 4, 6], "overwrit": 2, "ignor": 2, "none": [2, 4, 5, 6], "target_t": 2, "dask_util": 2, "dask_datafram": 2, "use_progress": 2, "true": [2, 6], "comput": 2, "dask": 2, "displai": 2, "progress": 2, "bar": 2, "bool": [2, 4, 6], "whether": [2, 4, 6], "dure": [2, 3, 7], "e": [2, 4, 7], "g": [2, 4, 7], "panda": [2, 6], "read_csv": 2, "csv": 2, "computed_df": 2, "head": 2, "column1": 2, "column2": 2, "4": [2, 6], "5": [2, 4, 6], "3": [2, 5, 6], "6": [2, 6], "n_worker": 2, "threads_per_work": 2, "client": 2, "worker": 2, "thread": 2, "per": [2, 6], "distribut": 2, "0x": 2, "data_process": 3, "dataframe_util": 3, "transform": [3, 7], "appli": [3, 6, 7], "set": [3, 5, 6, 7], "list": [3, 4, 5, 6, 7], "each": [3, 5, 6, 7], "should": [3, 7], "new_column": [3, 7], "new": [3, 5, 6, 7], "column": [3, 6, 7], "creat": [3, 7], "columns_to_us": [3, 7], "": [3, 4, 6, 7], "callabl": [3, 7], "pd": [3, 6, 7], "invalid": [3, 6, 7], "re": [3, 7], "occur": [3, 7], "after": [3, 7], "log": [3, 7], "def": [3, 7], "concat_column": [3, 7], "b": [3, 7], "f": [3, 7], "_": [3, 7], "col1": [3, 7], "col2": [3, 7], "c": [3, 7], "d": [3, 7], "col3": [3, 7], "lambda": [3, 7], "row": [3, 7], "transformed_df": [3, 7], "a_c": [3, 7], "b_d": [3, 7], "column_nam": [3, 7], "filter": [3, 5, 6, 7], "remov": [3, 5, 7], "check": [3, 6, 7], "out": [3, 7], "comment": [3, 7], "good": [3, 7], "product": [3, 5, 6, 7], "bad": [3, 6, 7], "servic": [3, 6, 7], "averag": [3, 4, 6, 7], "experi": [3, 7], "filtered_df": [3, 7], "embedding_util": 4, "token": [4, 5, 6], "model": [4, 5, 6], "gener": [4, 6], "vector": 4, "unigram": [4, 6], "bigram": [4, 6], "word2vec": 4, "train": 4, "np": 4, "ndarrai": 4, "gensim": 4, "king": 4, "queen": 4, "man": 4, "vector_s": 4, "100": 4, "min_count": 4, "epoch": 4, "unknown_token": 4, "shape": 4, "word2vec_model": 4, "save": 4, "directori": 4, "nativ": 4, "sentenc": [4, 5, 6], "world": [4, 6], "fals": [4, 6], "corpu": 4, "either": 4, "configur": [4, 5], "librari": 4, "addit": 4, "can": [4, 5, 6], "via": 4, "machin": 4, "learn": 4, "window": [4, 6], "wv": 4, "arrai": 4, "0123": 4, "0456": 4, "0789": 4, "dtype": [4, 6], "float32": 4, "text_preprocess": 5, "clean": 5, "input": [5, 6], "expand": [5, 6], "contract": [5, 6], "unwant": 5, "charact": 5, "normal": 5, "space": 5, "t": [5, 6], "parti": 5, "cannot": [5, 6], "spaci": [5, 6], "custom": 5, "languag": [5, 6], "nlp": [5, 6], "doc": 5, "text_input": [5, 6], "valid": [5, 6], "word": [5, 6], "count": [5, 6], "cleaned_text": 5, "believ": 5, "text_list": [5, 6], "love": [5, 6], "hi": 5, "tokens_list": 5, "phrase": [5, 6], "its": 5, "doesn": 5, "meet": 5, "criteria": 5, "lemmat": [5, 6], "document": 5, "exclud": [5, 6], "stop": [5, 6], "punctuat": 5, "am": [5, 6], "featur": [5, 6], "priorit": 5, "proper": 5, "noun": 5, "capit": 5, "john": 5, "york": 5, "text_process": 6, "text_util": 6, "positive_count": 6, "negative_count": 6, "combin": 6, "two": 6, "sentiment": 6, "score": 6, "singl": 6, "posit": 6, "neg": 6, "both": 6, "zero": 6, "float": 6, "25": 6, "contraction_map": 6, "m": 6, "input_list": 6, "n": 6, "gram": 6, "yield": 6, "iter": 6, "over": 6, "code": 6, "is_low": 6, "line": 6, "convert": 6, "lowercas": 6, "word_set": 6, "positive_word": 6, "happi": 6, "joy": 6, "delight": 6, "syllabl": 6, "valu": 6, "ha": 6, "syllable_count": 6, "beauti": 6, "spacy_token": 6, "perform": 6, "en_core_web_sm": 6, "non": 6, "empti": 6, "join": 6, "them": 6, "strip": 6, "els": 6, "text_analysi": 6, "input_word": 6, "negative_word": 6, "negation_word": 6, "polar": 6, "negat": 6, "total": 6, "sum": 6, "legaci": 6, "like": 6, "po": 6, "hate": 6, "dislik": 6, "never": 6, "weight": 6, "apply_weight": 6, "relev": 6, "preced": 6, "within": 6, "three": 6, "against": 6, "otherwis": 6, "fog": 6, "evalu": 6, "readabl": 6, "analyz": 6, "complex": 6, "simpl": 6, "unnecessarili": 6, "verbos": 6, "fog_scor": 6, "7": 6, "14": 6, "word_set_dict": 6, "topic": 6, "label": 6, "updat": 6, "section1": 6, "updated_df": 6, "matches_section1": 6, "sent_labels_section1": 6, "love_total_section1": 6, "love_sent_section1": 6, "float64": 6, "statist": 6, "uni": 6, "bi": 6, "stat": 6, "len_section1": 6, "raw_len_section1": 6, "bad_total_section1": 6, "love_stats_section1": 6, "bad_stats_section1": 6, "num_sents_section1": 6, "origin": 6, "veri": 6, "extrem": 6, "match_set": 6, "determin": 6, "suffix": 6, "rule": 6, "syllable_dict": 6, "cat": 6, "filenam": 6, "artifact": 6, "len": 6, "1500": 6, "very_happi": 6, "multipl": 6, "net": 6, "identifi": 6, "8": 6, "consid": 6, "occurr": 6, "defin": 6, "helper": 7, "extern": 7, "get_us": 7}, "objects": {"centralized_nlp_package.common_utils": [[1, 0, 0, "-", "date_utils"], [1, 0, 0, "-", "file_utils"], [1, 0, 0, "-", "string_utils"]], "centralized_nlp_package.common_utils.date_utils": [[1, 1, 1, "", "format_date"], [1, 1, 1, "", "get_date_range"]], "centralized_nlp_package.common_utils.file_utils": [[1, 1, 1, "", "load_content_from_txt"]], "centralized_nlp_package.common_utils.string_utils": [[1, 1, 1, "", "format_string_template"], [1, 1, 1, "", "query_constructor"]], "centralized_nlp_package.data_access": [[2, 0, 0, "-", "dask_utils"], [2, 0, 0, "-", "snowflake_utils"]], "centralized_nlp_package.data_access.dask_utils": [[2, 1, 1, "", "dask_compute_with_progress"], [2, 1, 1, "", "initialize_dask_client"]], "centralized_nlp_package.data_access.snowflake_utils": [[2, 1, 1, "", "execute_snowflake_query_spark"], [2, 1, 1, "", "execute_truncate_or_merge_query"], [2, 1, 1, "", "get_snowflake_connection_options"], [2, 1, 1, "", "retrieve_snowflake_private_key"], [2, 1, 1, "", "singleton"], [2, 1, 1, "", "with_spark_session"], [2, 1, 1, "", "write_dataframe_to_snowflake"]], "centralized_nlp_package.data_processing": [[3, 0, 0, "-", "dataframe_utils"]], "centralized_nlp_package.data_processing.dataframe_utils": [[3, 1, 1, "", "df_apply_transformations"], [3, 1, 1, "", "df_remove_rows_with_keywords"]], "centralized_nlp_package.embedding": [[4, 0, 0, "-", "embedding_utils"], [4, 0, 0, "-", "word2vec_model"]], "centralized_nlp_package.embedding.embedding_utils": [[4, 1, 1, "", "average_token_embeddings"]], "centralized_nlp_package.embedding.word2vec_model": [[4, 1, 1, "", "save_word2vec_model"], [4, 1, 1, "", "train_word2vec_model"]], "centralized_nlp_package.preprocessing": [[5, 0, 0, "-", "text_preprocessing"]], "centralized_nlp_package.preprocessing.text_preprocessing": [[5, 1, 1, "", "clean_text"], [5, 1, 1, "", "initialize_spacy"], [5, 1, 1, "", "preprocess_text"], [5, 1, 1, "", "preprocess_text_list"], [5, 1, 1, "", "remove_unwanted_phrases_and_validate"], [5, 1, 1, "", "tokenize_and_lemmatize_text"], [5, 1, 1, "", "tokenize_matched_words"]], "centralized_nlp_package.text_processing": [[6, 0, 0, "-", "text_analysis"], [6, 0, 0, "-", "text_utils"]], "centralized_nlp_package.text_processing.text_analysis": [[6, 1, 1, "", "calculate_polarity_score"], [6, 1, 1, "", "calculate_sentence_score"], [6, 1, 1, "", "check_negation"], [6, 1, 1, "", "fog_analysis_per_section"], [6, 1, 1, "", "fog_analysis_per_sentence"], [6, 1, 1, "", "generate_match_count"], [6, 1, 1, "", "generate_sentence_relevance_score"], [6, 1, 1, "", "generate_topic_statistics"], [6, 1, 1, "", "get_match_set"], [6, 1, 1, "", "is_complex"], [6, 1, 1, "", "load_word_set"], [6, 1, 1, "", "match_count"], [6, 1, 1, "", "merge_counts"], [6, 1, 1, "", "netscore"], [6, 1, 1, "", "polarity_score_per_section"], [6, 1, 1, "", "polarity_score_per_sentence"], [6, 1, 1, "", "tone_count_with_negation_check"], [6, 1, 1, "", "tone_count_with_negation_check_per_sentence"]], "centralized_nlp_package.text_processing.text_utils": [[6, 1, 1, "", "combine_sentiment_scores"], [6, 1, 1, "", "expand_contractions"], [6, 1, 1, "", "generate_ngrams"], [6, 1, 1, "", "load_set_from_txt"], [6, 1, 1, "", "load_syllable_counts"], [6, 1, 1, "", "tokenize_text"], [6, 1, 1, "", "validate_and_format_text"]], "centralized_nlp_package.utils": [[7, 0, 0, "-", "helpers"]], "centralized_nlp_package.utils.helpers": [[7, 1, 1, "", "df_apply_transformations"], [7, 1, 1, "", "df_remove_rows_with_keywords"], [7, 1, 1, "", "format_date"], [7, 1, 1, "", "format_string_template"], [7, 1, 1, "", "get_date_range"], [7, 1, 1, "", "load_file"], [7, 1, 1, "", "query_constructor"]]}, "objtypes": {"0": "py:module", "1": "py:function"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"]}, "titleterms": {"welcom": 0, "central": 0, "nlp": 0, "packag": 0, "": 0, "document": 0, "content": 0, "indic": 0, "tabl": 0, "common": 1, "util": [1, 7], "data": [2, 3], "access": 2, "process": [3, 6], "embed": 4, "preprocess": 5, "text": 6}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1, "sphinx": 58}, "alltitles": {"Welcome to Centralized NLP Package\u2019s documentation!": [[0, "welcome-to-centralized-nlp-package-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Common Utils": [[1, "module-centralized_nlp_package.common_utils.date_utils"]], "Data Access": [[2, "module-centralized_nlp_package.data_access.snowflake_utils"]], "Data Processing": [[3, "module-centralized_nlp_package.data_processing.dataframe_utils"]], "Embedding": [[4, "module-centralized_nlp_package.embedding.embedding_utils"]], "Preprocessing": [[5, "module-centralized_nlp_package.preprocessing.text_preprocessing"]], "Text processing": [[6, "module-centralized_nlp_package.text_processing.text_utils"]], "Utilities": [[7, "module-centralized_nlp_package.utils.helpers"]]}, "indexentries": {"centralized_nlp_package.common_utils.date_utils": [[1, "module-centralized_nlp_package.common_utils.date_utils"]], "centralized_nlp_package.common_utils.file_utils": [[1, "module-centralized_nlp_package.common_utils.file_utils"]], "centralized_nlp_package.common_utils.string_utils": [[1, "module-centralized_nlp_package.common_utils.string_utils"]], "format_date() (in module centralized_nlp_package.common_utils.date_utils)": [[1, "centralized_nlp_package.common_utils.date_utils.format_date"]], "format_string_template() (in module centralized_nlp_package.common_utils.string_utils)": [[1, "centralized_nlp_package.common_utils.string_utils.format_string_template"]], "get_date_range() (in module centralized_nlp_package.common_utils.date_utils)": [[1, "centralized_nlp_package.common_utils.date_utils.get_date_range"]], "load_content_from_txt() (in module centralized_nlp_package.common_utils.file_utils)": [[1, "centralized_nlp_package.common_utils.file_utils.load_content_from_txt"]], "module": [[1, "module-centralized_nlp_package.common_utils.date_utils"], [1, "module-centralized_nlp_package.common_utils.file_utils"], [1, "module-centralized_nlp_package.common_utils.string_utils"], [2, "module-centralized_nlp_package.data_access.dask_utils"], [2, "module-centralized_nlp_package.data_access.snowflake_utils"], [3, "module-centralized_nlp_package.data_processing.dataframe_utils"], [4, "module-centralized_nlp_package.embedding.embedding_utils"], [4, "module-centralized_nlp_package.embedding.word2vec_model"], [5, "module-centralized_nlp_package.preprocessing.text_preprocessing"], [6, "module-centralized_nlp_package.text_processing.text_analysis"], [6, "module-centralized_nlp_package.text_processing.text_utils"], [7, "module-centralized_nlp_package.utils.helpers"]], "query_constructor() (in module centralized_nlp_package.common_utils.string_utils)": [[1, "centralized_nlp_package.common_utils.string_utils.query_constructor"]], "centralized_nlp_package.data_access.dask_utils": [[2, "module-centralized_nlp_package.data_access.dask_utils"]], "centralized_nlp_package.data_access.snowflake_utils": [[2, "module-centralized_nlp_package.data_access.snowflake_utils"]], "dask_compute_with_progress() (in module centralized_nlp_package.data_access.dask_utils)": [[2, "centralized_nlp_package.data_access.dask_utils.dask_compute_with_progress"]], "execute_snowflake_query_spark() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.execute_snowflake_query_spark"]], "execute_truncate_or_merge_query() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.execute_truncate_or_merge_query"]], "get_snowflake_connection_options() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.get_snowflake_connection_options"]], "initialize_dask_client() (in module centralized_nlp_package.data_access.dask_utils)": [[2, "centralized_nlp_package.data_access.dask_utils.initialize_dask_client"]], "retrieve_snowflake_private_key() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.retrieve_snowflake_private_key"]], "singleton() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.singleton"]], "with_spark_session() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.with_spark_session"]], "write_dataframe_to_snowflake() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.write_dataframe_to_snowflake"]], "centralized_nlp_package.data_processing.dataframe_utils": [[3, "module-centralized_nlp_package.data_processing.dataframe_utils"]], "df_apply_transformations() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.df_apply_transformations"]], "df_remove_rows_with_keywords() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.df_remove_rows_with_keywords"]], "average_token_embeddings() (in module centralized_nlp_package.embedding.embedding_utils)": [[4, "centralized_nlp_package.embedding.embedding_utils.average_token_embeddings"]], "centralized_nlp_package.embedding.embedding_utils": [[4, "module-centralized_nlp_package.embedding.embedding_utils"]], "centralized_nlp_package.embedding.word2vec_model": [[4, "module-centralized_nlp_package.embedding.word2vec_model"]], "save_word2vec_model() (in module centralized_nlp_package.embedding.word2vec_model)": [[4, "centralized_nlp_package.embedding.word2vec_model.save_word2vec_model"]], "train_word2vec_model() (in module centralized_nlp_package.embedding.word2vec_model)": [[4, "centralized_nlp_package.embedding.word2vec_model.train_word2vec_model"]], "centralized_nlp_package.preprocessing.text_preprocessing": [[5, "module-centralized_nlp_package.preprocessing.text_preprocessing"]], "clean_text() (in module centralized_nlp_package.preprocessing.text_preprocessing)": [[5, "centralized_nlp_package.preprocessing.text_preprocessing.clean_text"]], "initialize_spacy() (in module centralized_nlp_package.preprocessing.text_preprocessing)": [[5, "centralized_nlp_package.preprocessing.text_preprocessing.initialize_spacy"]], "preprocess_text() (in module centralized_nlp_package.preprocessing.text_preprocessing)": [[5, "centralized_nlp_package.preprocessing.text_preprocessing.preprocess_text"]], "preprocess_text_list() (in module centralized_nlp_package.preprocessing.text_preprocessing)": [[5, "centralized_nlp_package.preprocessing.text_preprocessing.preprocess_text_list"]], "remove_unwanted_phrases_and_validate() (in module centralized_nlp_package.preprocessing.text_preprocessing)": [[5, "centralized_nlp_package.preprocessing.text_preprocessing.remove_unwanted_phrases_and_validate"]], "tokenize_and_lemmatize_text() (in module centralized_nlp_package.preprocessing.text_preprocessing)": [[5, "centralized_nlp_package.preprocessing.text_preprocessing.tokenize_and_lemmatize_text"]], "tokenize_matched_words() (in module centralized_nlp_package.preprocessing.text_preprocessing)": [[5, "centralized_nlp_package.preprocessing.text_preprocessing.tokenize_matched_words"]], "calculate_polarity_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.calculate_polarity_score"]], "calculate_sentence_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.calculate_sentence_score"]], "centralized_nlp_package.text_processing.text_analysis": [[6, "module-centralized_nlp_package.text_processing.text_analysis"]], "centralized_nlp_package.text_processing.text_utils": [[6, "module-centralized_nlp_package.text_processing.text_utils"]], "check_negation() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.check_negation"]], "combine_sentiment_scores() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.combine_sentiment_scores"]], "expand_contractions() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.expand_contractions"]], "fog_analysis_per_section() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.fog_analysis_per_section"]], "fog_analysis_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.fog_analysis_per_sentence"]], "generate_match_count() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.generate_match_count"]], "generate_ngrams() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.generate_ngrams"]], "generate_sentence_relevance_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.generate_sentence_relevance_score"]], "generate_topic_statistics() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.generate_topic_statistics"]], "get_match_set() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.get_match_set"]], "is_complex() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.is_complex"]], "load_set_from_txt() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.load_set_from_txt"]], "load_syllable_counts() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.load_syllable_counts"]], "load_word_set() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.load_word_set"]], "match_count() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.match_count"]], "merge_counts() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.merge_counts"]], "netscore() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.netscore"]], "polarity_score_per_section() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.polarity_score_per_section"]], "polarity_score_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.polarity_score_per_sentence"]], "tokenize_text() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.tokenize_text"]], "tone_count_with_negation_check() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.tone_count_with_negation_check"]], "tone_count_with_negation_check_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.tone_count_with_negation_check_per_sentence"]], "validate_and_format_text() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.validate_and_format_text"]], "centralized_nlp_package.utils.helpers": [[7, "module-centralized_nlp_package.utils.helpers"]], "df_apply_transformations() (in module centralized_nlp_package.utils.helpers)": [[7, "centralized_nlp_package.utils.helpers.df_apply_transformations"]], "df_remove_rows_with_keywords() (in module centralized_nlp_package.utils.helpers)": [[7, "centralized_nlp_package.utils.helpers.df_remove_rows_with_keywords"]], "format_date() (in module centralized_nlp_package.utils.helpers)": [[7, "centralized_nlp_package.utils.helpers.format_date"]], "format_string_template() (in module centralized_nlp_package.utils.helpers)": [[7, "centralized_nlp_package.utils.helpers.format_string_template"]], "get_date_range() (in module centralized_nlp_package.utils.helpers)": [[7, "centralized_nlp_package.utils.helpers.get_date_range"]], "load_file() (in module centralized_nlp_package.utils.helpers)": [[7, "centralized_nlp_package.utils.helpers.load_file"]], "query_constructor() (in module centralized_nlp_package.utils.helpers)": [[7, "centralized_nlp_package.utils.helpers.query_constructor"]]}})