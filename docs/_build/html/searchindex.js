Search.setIndex({"docnames": ["index", "source/common_utils", "source/data_access", "source/data_processing", "source/embedding", "source/model_utils", "source/nli_utils", "source/text_processing", "source/utils"], "filenames": ["index.rst", "source/common_utils.rst", "source/data_access.rst", "source/data_processing.rst", "source/embedding.rst", "source/model_utils.rst", "source/nli_utils.rst", "source/text_processing.rst", "source/utils.rst"], "titles": ["Welcome to Centralized NLP Package\u2019s documentation!", "Common Utils", "Data Access", "Data Processing", "Embedding", "Model Utils", "NLI utilities", "Text processing", "Utilities"], "terms": {"data": [0, 1, 5, 6, 7], "access": [0, 5, 6], "execute_truncate_or_merge_queri": [0, 2], "read_from_snowflak": [0, 2], "write_dataframe_to_snowflak": [0, 2], "process": [0, 5, 6], "check_pd_dataframe_for_record": [0, 3], "concatenate_and_reset_index": [0, 3], "df_apply_transform": [0, 3], "df_remove_rows_with_keyword": [0, 3], "save_report": [0, 3], "dask_compute_with_progress": [0, 3], "initialize_dask_cli": [0, 3], "check_spark_dataframe_for_record": [0, 3], "convert_columns_to_timestamp": [0, 3], "define_structur": [0, 3], "equivalent_typ": [0, 3], "get_default_dtype_map": [0, 3], "initialize_spark_sess": [0, 3], "pandas_to_spark": [0, 3], "sparkdf_apply_transform": [0, 3], "text": [0, 1, 3], "clean_text": [0, 7], "initialize_spaci": [0, 7], "preprocess_text": [0, 7], "preprocess_text_list": [0, 7], "remove_unwanted_phrases_and_valid": [0, 7], "tokenize_and_lemmatize_text": [0, 7], "tokenize_matched_word": [0, 7], "combine_sentiment_scor": [0, 7], "expand_contract": [0, 7], "generate_ngram": [0, 7], "load_set_from_txt": [0, 7], "load_syllable_count": [0, 7], "tokenize_text": [0, 7], "validate_and_format_text": [0, 7], "calculate_net_scor": [0, 7], "calculate_polarity_scor": [0, 7], "calculate_sentence_scor": [0, 7], "check_neg": [0, 7], "fog_analysis_per_sect": [0, 7], "fog_analysis_per_sent": [0, 7], "generate_match_count": [0, 7], "generate_sentence_relevance_scor": [0, 7], "generate_topic_statist": [0, 7], "get_match_set": [0, 7], "is_complex": [0, 7], "load_word_set": [0, 7], "match_count": [0, 7], "merge_count": [0, 7], "polarity_score_per_sect": [0, 7], "polarity_score_per_sent": [0, 7], "tone_count_with_negation_check": [0, 7], "tone_count_with_negation_check_per_sent": [0, 7], "embed": 0, "average_token_embed": [0, 4], "save_word2vec_model": [0, 4], "train_word2vec_model": [0, 4], "model": [0, 4, 6, 7], "util": [0, 2, 3], "get_best_models_by_param": [0, 5], "get_best_models_by_tag": [0, 5], "list_available_model": [0, 5], "debertamodel": [0, 5], "finbertmodel": [0, 5], "get_model": [0, 5], "nli": [0, 5], "datatrainingargu": [0, 6], "modelargu": [0, 6], "compute_metr": [0, 6], "evaluate_nli_models_from_path": [0, 6], "evaluate_nli_models_mlflow": [0, 6], "get_compute_metr": [0, 6], "get_nli_model_metr": [0, 6], "run_finetun": [0, 6], "run_glu": [0, 5, 6], "initialize_nli_infer_pipelin": [0, 6], "determine_environ": [0, 8], "load_config_from_fil": [0, 8], "common": [0, 3], "format_d": [0, 1], "get_current_date_str": [0, 1], "get_date_rang": [0, 1], "load_content_from_txt": [0, 1], "format_string_templ": [0, 1], "query_constructor": [0, 1], "index": [0, 3, 5, 7], "modul": [0, 5], "search": 0, "page": 0, "centralized_nlp_packag": [1, 2, 3, 4, 5, 6, 7, 8], "common_util": 1, "date_util": 1, "date": [1, 3], "sourc": [1, 2, 3, 4, 5, 6, 7, 8], "format": [1, 3, 4, 7], "datetim": [1, 3], "object": [1, 3, 5, 6, 7, 8], "string": [1, 3, 7], "yyyi": [1, 3], "mm": [1, 3], "dd": [1, 3], "thi": [1, 2, 3, 4, 5, 6, 7, 8], "function": [1, 2, 3, 4, 5, 6, 7, 8], "take": [1, 3, 6, 8], "return": [1, 2, 3, 4, 5, 6, 7, 8], "its": [1, 5, 6, 7], "represent": 1, "paramet": [1, 2, 3, 4, 5, 6, 7, 8], "The": [1, 2, 3, 4, 5, 6, 7, 8], "type": [1, 2, 3, 4, 5, 6, 7, 8], "str": [1, 2, 3, 4, 5, 6, 7, 8], "exampl": [1, 2, 3, 4, 5, 6, 7, 8], "from": [1, 2, 3, 4, 5, 6, 7], "import": [1, 2, 3, 4, 6, 7], "2023": [1, 3], "9": [1, 3], "15": [1, 4], "print": [1, 3, 4, 5, 6, 7], "09": [1, 3], "retriev": [1, 3, 5, 6], "current": [1, 7], "yyyymmdd": 1, "without": [1, 6], "ani": [1, 3, 4, 5, 6, 7], "separ": [1, 5], "follow": [1, 3, 6], "pattern": 1, "current_d": 1, "20250127": 1, "years_back": 1, "0": [1, 3, 4, 5, 6, 7], "months_back": 1, "calcul": [1, 3, 6, 7], "rang": [1, 3], "base": [1, 3, 5, 6, 7, 8], "minu": 1, "specifi": [1, 2, 3, 4, 5, 6, 7, 8], "year": 1, "month": 1, "comput": [1, 3, 5, 6], "start": [1, 5], "end": 1, "subtract": 1, "given": [1, 2, 3, 5, 6, 7], "number": [1, 3, 5, 6, 7], "ar": [1, 2, 3, 4, 5, 6, 7], "repres": [1, 5], "first": [1, 5], "dai": 1, "respect": 1, "int": [1, 3, 5, 6, 7], "option": [1, 2, 3, 4, 5, 6, 7, 8], "go": [1, 7], "back": [1, 3, 6], "default": [1, 2, 3, 4, 5, 6, 7], "A": [1, 2, 3, 4, 5, 6, 7], "tupl": [1, 3, 5, 6, 7], "contain": [1, 3, 5, 6, 7], "start_dat": 1, "end_dat": 1, "1": [1, 3, 4, 5, 6, 7], "2": [1, 3, 7], "2022": 1, "08": [1, 3], "01": [1, 3, 5, 6], "10": [1, 3, 4, 7], "file_util": 1, "file_path": [1, 7, 8], "read": [1, 7], "entir": 1, "content": [1, 7, 8], "file": [1, 3, 4, 5, 6, 7, 8], "path": [1, 3, 4, 5, 6, 7, 8], "rais": [1, 2, 3, 5, 6, 7, 8], "filesnotloadedexcept": [1, 7], "If": [1, 2, 3, 5, 6, 7, 8], "i": [1, 2, 3, 5, 6, 7, 8], "found": [1, 3, 5, 7], "sampl": [1, 3, 5, 6, 7], "txt": [1, 7], "string_util": 1, "templat": 1, "kwarg": [1, 4, 5], "construct": [1, 3], "replac": 1, "placehold": 1, "provid": [1, 2, 3, 4, 5, 6, 7, 8], "keyword": [1, 3, 4, 5], "argument": [1, 3, 4, 5, 6, 8], "form": 1, "kei": [1, 3, 5, 6, 7], "variabl": 1, "valueerror": [1, 3, 5, 6, 8], "doe": [1, 3, 5, 6], "have": [1, 5, 6], "correspond": [1, 3, 5, 6], "hello": [1, 3, 4, 7], "name": [1, 2, 3, 5, 6, 7, 8], "todai": [1, 6], "alic": [1, 3], "mondai": 1, "query_input": 1, "queri": [1, 2], "load": [1, 5, 6, 7, 8], "us": [1, 2, 3, 4, 5, 6, 7, 8], "union": [1, 3, 7], "o": [1, 3], "pathlik": 1, "itself": 1, "substitut": 1, "do": [1, 7], "match": [1, 7], "exist": [1, 2, 3, 4, 5, 6], "select": [1, 2, 5], "user": [1, 7, 8], "where": [1, 2, 3, 5, 7], "signup_d": 1, "AND": 1, "12": [1, 3, 7], "31": 1, "data_access": [2, 3], "snowflake_util": 2, "databas": 2, "schema": [2, 3], "execut": [2, 5, 6], "truncat": [2, 3, 6], "merg": [2, 7], "sql": [2, 3], "snowflak": 2, "run": [2, 5, 6], "either": [2, 3, 4], "against": [2, 7], "establish": 2, "spark": [2, 3], "session": [2, 3], "It": [2, 3, 5, 6], "ensur": [2, 3, 4, 5, 6, 7], "within": [2, 5, 7], "correct": 2, "context": 2, "typic": 2, "statement": 2, "target": 2, "confirm": 2, "messag": [2, 3, 5], "indic": [2, 6, 7], "success": 2, "complet": [2, 5], "oper": [2, 3, 5], "except": [2, 3, 5, 6, 7], "an": [2, 3, 4, 5, 6, 7, 8], "error": [2, 5, 6, 7, 8], "tabl": 2, "analytics_db": 2, "public": 2, "sales_data": 2, "result": [2, 3, 6, 7], "datafram": [2, 3, 6, 7], "set": [2, 3, 5, 6, 7], "appropri": [2, 3, 5, 6], "connect": 2, "df": [2, 3, 7], "show": [2, 3], "table_nam": 2, "mode": 2, "append": 2, "write": 2, "allow": [2, 3, 6, 7], "correctli": 2, "direct": 2, "intend": 2, "locat": 2, "written": 2, "behavior": 2, "alreadi": [2, 3], "overwrit": [2, 3, 5, 6], "new": [2, 3, 7], "throw": 2, "ignor": 2, "none": [2, 3, 4, 5, 6, 7], "data_process": 3, "dataframe_util": 3, "datetime_col": 3, "parsed_datetime_eastern_tz": 3, "exit_on_empti": 3, "true": [3, 6, 7], "exit_method": 3, "both": [3, 7], "check": [3, 7], "non": [3, 7], "empti": [3, 5, 7], "relev": [3, 5, 6, 7], "inform": 3, "exit": 3, "program": 3, "pd": [3, 6, 7], "column": [3, 6, 7], "determin": [3, 5, 6, 7, 8], "span": 3, "bool": [3, 4, 5, 6, 7], "whether": [3, 4, 5, 6, 7], "method": [3, 5], "dbutil": 3, "drop_column": 3, "concaten": 3, "multipl": [3, 7], "panda": [3, 6, 7], "reset": 3, "drop": 3, "old": 3, "perform": [3, 5, 6, 7], "step": 3, "along": [3, 6], "row": 3, "list": [3, 4, 5, 6, 7], "after": [3, 5, 6], "input": [3, 5, 6, 7], "keyerror": [3, 6], "rdf": 3, "b": 3, "3": [3, 6, 7], "4": [3, 5, 6, 7], "cdf": 3, "5": [3, 4, 5, 6, 7], "6": [3, 6, 7], "7": [3, 6, 7], "8": [3, 6, 7], "sdf": 3, "11": 3, "concatdf": 3, "transform": [3, 5, 6], "appli": [3, 6, 7], "each": [3, 5, 6, 7], "should": 3, "new_column": 3, "creat": 3, "columns_to_us": 3, "": [3, 4, 5, 6, 7], "func": 3, "callabl": [3, 6], "invalid": [3, 7], "re": 3, "occur": [3, 6], "dure": [3, 5, 6, 7], "log": [3, 5, 6], "def": 3, "concat_column": 3, "f": [3, 5], "_": 3, "col1": 3, "col2": 3, "c": 3, "d": 3, "col3": 3, "lambda": 3, "transformed_df": 3, "a_c": 3, "b_d": 3, "column_nam": 3, "filter": [3, 7], "remov": [3, 7], "out": 3, "comment": 3, "good": [3, 7], "product": [3, 7], "bad": [3, 7], "servic": [3, 7], "averag": [3, 4, 7], "experi": [3, 5, 6], "filtered_df": 3, "save": [3, 4, 5, 6], "report": 3, "csv": [3, 5, 6], "parquet": 3, "extens": 3, "infer": [3, 5, 6], "support": [3, 5, 6], "importerror": 3, "requir": [3, 6, 7], "engin": 3, "instal": 3, "usag": [3, 6, 7], "save_report_modul": 3, "assum": 3, "py": [3, 5], "bob": 3, "charli": 3, "score": [3, 5, 6, 7], "85": [3, 5, 6], "92": 3, "78": 3, "dask_util": 3, "dask_datafram": 3, "use_progress": 3, "dask": 3, "displai": 3, "progress": 3, "bar": 3, "e": [3, 4, 5, 6, 8], "g": [3, 4, 5, 6, 8], "read_csv": 3, "computed_df": 3, "head": [3, 6], "column1": 3, "column2": 3, "n_worker": 3, "32": [3, 6], "threads_per_work": 3, "initi": [3, 4, 5, 6, 7, 8], "client": 3, "worker": [3, 4], "thread": 3, "per": [3, 7], "instanc": [3, 5, 6], "distribut": 3, "0x": 3, "spark_util": 3, "spark_df": 3, "record": 3, "verifi": 3, "presenc": 3, "present": [3, 6], "minimum": [3, 7], "maximum": [3, 5, 6, 7], "count": [3, 7], "warn": [3, 5], "notebook": 3, "valid": [3, 5, 6, 7], "pyspark": 3, "sparksess": 3, "builder": 3, "appnam": 3, "dataframecheckexampl": 3, "getorcr": 3, "00": 3, "02": [3, 6], "30": 3, "createdatafram": 3, "ha": [3, 7], "columns_format": 3, "convert": [3, 6, 7], "timestamp": 3, "to_timestamp": 3, "dictionari": [3, 5, 6, 7], "valu": [3, 5, 6, 7], "_t": 3, "suffix": [3, 7], "dict": [3, 5, 6, 7], "rubric": 3, "report_d": 3, "hh": 3, "ss": 3, "event_datetime_utc": 3, "fals": [3, 4, 6, 7], "timestampconvers": 3, "13": 3, "45": 3, "columns_to_convert": 3, "converted_df": 3, "printschema": 3, "root": 3, "nullabl": 3, "pandas_dtyp": 3, "column_map": 3, "structfield": 3, "structtyp": 3, "dtype": [3, 4, 7], "datatyp": 3, "map": 3, "identifi": [3, 5, 6, 7], "custom": [3, 6, 7], "field": 3, "ag": 3, "int64": 3, "longtyp": 3, "custom_map": 3, "price": 3, "float": [3, 5, 6, 7], "floattyp": 3, "priorit": [3, 7], "specif": [3, 6, 7], "fall": 3, "still": 3, "unresolv": 3, "stringtyp": 3, "predefin": [3, 7], "facilit": 3, "convers": 3, "definit": 3, "dtype_map": 3, "app_nam": 3, "optimized_nli_infer": 3, "shuffle_partit": 3, "200": 3, "gpu_amount": 3, "task_gpu_amount": 3, "executor_memori": 3, "4g": 3, "driver_memori": 3, "2g": 3, "executor_cor": 3, "memory_overhead": 3, "512m": 3, "dynamic_alloc": 3, "configur": [3, 4, 5, 6, 7, 8], "applic": 3, "partit": 3, "shuffl": 3, "amount": 3, "gpu": [3, 5, 6], "resourc": 3, "alloc": 3, "executor": 3, "task": [3, 5, 6], "memori": [3, 6], "driver": 3, "cpu": [3, 5, 6], "core": 3, "overhead": 3, "enabl": [3, 6], "dynam": [3, 6], "fail": [3, 5], "spark_sess": 3, "mysparkapp": 3, "100": [3, 4], "8g": 3, "pandas_df": 3, "column_type_map": 3, "customiz": 3, "defin": [3, 6, 7], "int32": 3, "integertyp": 3, "float64": [3, 7], "doubletyp": 3, "float32": [3, 4], "booleantyp": 3, "datetime64": 3, "n": [3, 7], "timestamptyp": 3, "timedelta": 3, "arr": 3, "arraytyp": 3, "long": 3, "doubl": 3, "maptyp": 3, "filt_md": 3, "arr_str": 3, "stat": [3, 7], "map_str_int": 3, "activ": 3, "issu": [3, 5, 6, 8], "error_on_miss": 3, "seri": 3, "overwritten": 3, "new_column_nam": 3, "input_column": 3, "transformation_funct": 3, "pass": [3, 5], "one": [3, 6], "more": 3, "miss": 3, "skip": 3, "all": [3, 5, 6], "typeerror": [3, 7], "structur": 3, "udf": 3, "transformationexampl": 3, "world": [3, 4, 7], "text_column": 3, "date_column": 3, "to_upp": 3, "upper": 3, "els": [3, 7], "extract_year": 3, "date_str": 3, "split": 3, "to_upper_udf": 3, "extract_year_udf": 3, "text_upp": 3, "year_extract": 3, "combin": [3, 7], "concat": 3, "lit": 3, "apply_transform": 3, "output": [3, 5, 6], "embedding_util": 4, "token": [4, 5, 6, 7], "gener": [4, 6, 7], "vector": 4, "unigram": [4, 7], "bigram": [4, 7], "word2vec": 4, "train": [4, 5, 6], "np": [4, 6], "ndarrai": [4, 6], "gensim": 4, "king": 4, "queen": 4, "man": 4, "vector_s": 4, "min_count": 4, "epoch": [4, 5], "unknown_token": 4, "shape": 4, "word2vec_model": 4, "directori": [4, 5, 6, 8], "nativ": 4, "sentenc": [4, 7], "300": 4, "window": [4, 7], "16": [4, 5, 6], "corpu": 4, "librari": [4, 6], "addit": [4, 5, 7], "can": [4, 6, 7], "via": [4, 6], "machin": 4, "learn": [4, 5], "wv": 4, "arrai": [4, 6], "0123": 4, "0456": 4, "0789": 4, "model_util": 5, "model_selector": 5, "experiment_nam": [5, 6], "param": 5, "metric": [5, 6], "accuraci": [5, 6], "best": [5, 6], "uniqu": 5, "mlflow": [5, 6], "group": 5, "highest": 5, "which": [5, 6], "evalu": [5, 6, 7], "entiti": [5, 7], "mlflowexcept": [5, 6], "best_run": 5, "nli_experi": [5, 6], "learning_r": [5, 6], "f1_score": [5, 6], "id": [5, 6], "info": 5, "run_id": [5, 6], "rate": 5, "f1": [5, 6], "tag": [5, 6, 7], "model_vers": 5, "version": [5, 6], "avail": [5, 6, 7], "sort": [5, 6], "descend": 5, "order": 5, "them": [5, 6, 7], "compil": [5, 6], "detail": 5, "higher": [5, 7], "appear": 5, "run_nam": [5, 6], "class": [5, 6], "model_nam": [5, 6], "model_path": [5, 6], "devic": [5, 6], "basemodel": 5, "special": 5, "handl": 5, "deberta": [5, 6], "natur": [5, 6], "languag": [5, 6, 7], "inherit": 5, "zero": [5, 6, 7], "shot": [5, 6], "classif": [5, 6], "hug": [5, 6], "face": [5, 6], "pipelin": [5, 6], "integr": 5, "track": 5, "pre": [5, 6], "validation_fil": [5, 6], "dataset": [5, 6], "filenotfounderror": [5, 6], "train_fil": [5, 6], "param_dict": [5, 6], "output_dir": [5, 6], "eval_entailment_thresold": [5, 6], "hyperparamet": [5, 6], "orchestr": [5, 6], "up": [5, 6], "invok": 5, "includ": [5, 6, 7, 8], "cache_dir": [5, 6], "cach": [5, 6], "num_train_epoch": [5, 6], "per_device_train_batch_s": [5, 6], "batch": 5, "size": [5, 6], "per_device_eval_batch_s": [5, 6], "task_nam": [5, 6], "max_seq_length": [5, 6], "sequenc": [5, 6], "length": [5, 6, 7], "pad_to_max_length": [5, 6], "pad": [5, 6], "overwrite_cach": [5, 6], "max_train_sampl": [5, 6], "max_eval_sampl": [5, 6], "max_predict_sampl": [5, 6], "predict": [5, 6], "threshold": [5, 6], "entail": [5, 6], "posit": [5, 6, 7], "associ": 5, "automodelforsequenceclassif": [5, 6], "autotoken": 5, "finbert": [5, 6], "leverag": 5, "subprocess": 5, "call": [5, 7], "script": 5, "extern": 5, "python": [5, 6], "n_epoch": 5, "optim": [5, 6], "weight_decai": [5, 6], "weight": [5, 7], "decai": 5, "train_batch_s": [5, 6], "eval_batch_s": [5, 6], "calledprocesserror": 5, "uncas": [5, 6], "hyperparam": 5, "finbert_output": 5, "3e": [5, 6], "train_finbert": 5, "validation_finbert": 5, "convent": 5, "onli": [5, 6, 7], "instanti": 5, "v3": [5, 6], "mlflow_util": 5, "unknown": [5, 6], "traceback": 5, "most": 5, "recent": 5, "last": 5, "nli_util": 6, "dataset_nam": 6, "dataset_config_nam": 6, "128": 6, "test_fil": 6, "pertain": 6, "total": [6, 7], "preprocess": [6, 7], "debug": 6, "quicker": 6, "json": 6, "test": [6, 7], "data_arg": 6, "mnli": 6, "model_name_or_path": 6, "dbf": 6, "mnt": 6, "access_work": 6, "uc25": 6, "huggingfac": 6, "larg": 6, "zeroshot": 6, "v2": 6, "config_nam": 6, "tokenizer_nam": 6, "use_fast_token": 6, "model_revis": 6, "main": 6, "trust_remote_cod": 6, "ignore_mismatched_s": 6, "pretrain": 6, "co": 6, "config": [6, 7, 8], "same": 6, "store": 6, "download": 6, "fast": 6, "branch": 6, "commit": 6, "http": [6, 7], "bearer": 6, "author": 6, "remot": 6, "hub": 6, "own": 6, "whose": 6, "dimens": 6, "differ": 6, "model_arg": 6, "bert": 6, "pred": 6, "label": [6, 7], "entailment_threshold": 6, "standard": 6, "raw": 6, "binari": 6, "variou": 6, "precis": 6, "recal": 6, "roc": 6, "auc": 6, "otherwis": [6, 7], "abov": 6, "consid": [6, 7], "origin": [6, 7], "roc_auc": 6, "mismatch": 6, "csv_path": 6, "compar": 6, "fine": 6, "tune": 6, "iter": [6, 7], "through": 6, "must": 6, "sentence1": 6, "sentence2": 6, "deberta_finetun": 6, "finbert_finetun": 6, "evaluation_result": 6, "eval_dataset": 6, "model_family_nam": 6, "time_taken_second": 6, "120": 6, "2e": 6, "05": 6, "80": 6, "75": 6, "77": 6, "83": 6, "150": 6, "88": 6, "82": 6, "81": 6, "12345abcdef": 6, "deberta_run_v1": 6, "67890ghijkl": 6, "finbert_run_v2": 6, "is_regress": 6, "tailor": 6, "regress": 6, "For": [6, 7], "treat": 6, "qqp": 6, "evalpredict": 6, "compute_metrics_fn": 6, "eval_pr": 6, "label_id": 6, "nli_pipelin": 6, "eval_df": 6, "encapsul": 6, "modelevaluationresult": 6, "dataclass": 6, "facebook": 6, "bart": 6, "eval_data": 6, "sky": 6, "blue": 6, "cat": [6, 7], "anim": 6, "sunni": 6, "dog": 6, "mammal": 6, "123": 6, "base_model_path": 6, "training_arg": 6, "glue": 6, "prepar": 6, "relat": 6, "trainingargu": 6, "do_train": 6, "do_ev": 6, "report_to": 6, "nli_infer": 6, "enable_quant": 6, "quantiz": 6, "faster": 6, "reduc": 6, "local": 6, "flag": 6, "linear": 6, "layer": 6, "footprint": 6, "potenti": 6, "increas": 6, "speed": 6, "especi": 6, "readi": [6, 7], "inaccess": 6, "incompat": 6, "other": 6, "nli_finetun": 6, "movi": 6, "wa": 6, "fantast": 6, "love": [6, 7], "candidate_label": 6, "neg": [6, 7], "neutral": 6, "hypothesis_templ": 6, "sentiment": [6, 7], "review": 6, "95": 6, "04": 6, "note": 6, "lower": 6, "torch": 6, "qint8": 6, "lead": 6, "time": 6, "particularli": 6, "benefici": 6, "when": 6, "deploi": 6, "environ": [6, 8], "howev": 6, "mai": 6, "slightli": 6, "degrad": 6, "automat": 6, "detect": [6, 8], "hardwar": 6, "compat": 6, "like": [6, 7], "analysi": 6, "suitabl": 6, "text_process": 7, "text_preprocess": 7, "clean": 7, "expand": 7, "contract": 7, "unwant": 7, "charact": 7, "normal": 7, "space": 7, "t": 7, "parti": 7, "cannot": [7, 8], "en_core_web_sm": 7, "max_length": 7, "1000000000": 7, "exclude_stop_word": 7, "bottom": 7, "top": 7, "spaci": 7, "adjust": 7, "stop": 7, "word": 7, "exclud": 7, "en_core_web_md": 7, "en_core_web_lg": 7, "etc": 7, "larger": 7, "what": 7, "insignific": 7, "nlp": 7, "2000000000": 7, "doc": 7, "text_input": 7, "cleaned_text": 7, "believ": 7, "text_list": 7, "hi": 7, "tokens_list": 7, "min_word_length": 7, "cleanup_phras": 7, "greeting_phras": 7, "greet": 7, "phrase": 7, "thank": 7, "you": 7, "earn": 7, "releas": 7, "confer": 7, "morn": 7, "afternoon": 7, "even": 7, "unexpect": 7, "pos_exclud": 7, "ent_type_exclud": 7, "lemmat": 7, "document": 7, "punctuat": 7, "part": 7, "speech": 7, "po": 7, "refer": 7, "io": 7, "linguist": 7, "featur": 7, "www": 7, "restack": 7, "p": 7, "recognit": 7, "answer": 7, "ai": 7, "am": 7, "appl": 7, "look": 7, "bui": 7, "u": 7, "k": 7, "startup": 7, "billion": 7, "verb": 7, "org": 7, "proper": 7, "noun": 7, "capit": 7, "john": 7, "york": 7, "text_util": 7, "positive_count": 7, "negative_count": 7, "two": 7, "singl": 7, "25": 7, "contraction_map": 7, "m": 7, "input_list": 7, "gram": 7, "yield": 7, "over": 7, "code": 7, "is_low": 7, "line": 7, "lowercas": 7, "word_set": 7, "positive_word": 7, "happi": 7, "joy": 7, "delight": 7, "syllabl": 7, "syllable_count": 7, "beauti": 7, "spacy_token": 7, "join": 7, "strip": 7, "text_analysi": 7, "net": 7, "input_word": 7, "negative_word": 7, "negation_word": 7, "polar": 7, "negat": 7, "sum": 7, "legaci": 7, "hate": 7, "dislik": 7, "never": 7, "apply_weight": 7, "preced": 7, "three": 7, "fog": 7, "readabl": 7, "analyz": 7, "complex": 7, "simpl": 7, "unnecessarili": 7, "verbos": 7, "fog_scor": 7, "14": 7, "word_set_dict": 7, "topic": 7, "updat": 7, "section1": 7, "updated_df": 7, "matches_section1": 7, "sent_labels_section1": 7, "love_total_section1": 7, "love_sent_section1": 7, "statist": 7, "uni": 7, "bi": 7, "len_section1": 7, "raw_len_section1": 7, "bad_total_section1": 7, "love_stats_section1": 7, "bad_stats_section1": 7, "num_sents_section1": 7, "veri": 7, "extrem": 7, "match_set": 7, "rule": 7, "syllable_dict": 7, "filenam": [7, 8], "artifact": 7, "len": 7, "1500": 7, "very_happi": 7, "account": 7, "occurr": 7, "helper": 8, "provided_env": 8, "quant": 8, "auto": 8, "databrick": 8, "workspac": 8, "dev": 8, "stg": 8, "prod": 8, "hydra": 8, "full": 8, "extract": 8, "compos": 8, "yaml": 8, "dictconfig": 8, "runtimeerror": 8, "get_config": 8, "your": 8}, "objects": {"centralized_nlp_package.common_utils": [[1, 0, 0, "-", "date_utils"], [1, 0, 0, "-", "file_utils"], [1, 0, 0, "-", "string_utils"]], "centralized_nlp_package.common_utils.date_utils": [[1, 1, 1, "", "format_date"], [1, 1, 1, "", "get_current_date_str"], [1, 1, 1, "", "get_date_range"]], "centralized_nlp_package.common_utils.file_utils": [[1, 1, 1, "", "load_content_from_txt"]], "centralized_nlp_package.common_utils.string_utils": [[1, 1, 1, "", "format_string_template"], [1, 1, 1, "", "query_constructor"]], "centralized_nlp_package.data_access": [[2, 0, 0, "-", "snowflake_utils"]], "centralized_nlp_package.data_access.snowflake_utils": [[2, 1, 1, "", "execute_truncate_or_merge_query"], [2, 1, 1, "", "read_from_snowflake"], [2, 1, 1, "", "write_dataframe_to_snowflake"]], "centralized_nlp_package.data_processing": [[3, 0, 0, "-", "dask_utils"], [3, 0, 0, "-", "dataframe_utils"], [3, 0, 0, "-", "spark_utils"]], "centralized_nlp_package.data_processing.dask_utils": [[3, 1, 1, "", "dask_compute_with_progress"], [3, 1, 1, "", "initialize_dask_client"]], "centralized_nlp_package.data_processing.dataframe_utils": [[3, 1, 1, "", "check_pd_dataframe_for_records"], [3, 1, 1, "", "concatenate_and_reset_index"], [3, 1, 1, "", "df_apply_transformations"], [3, 1, 1, "", "df_remove_rows_with_keywords"], [3, 1, 1, "", "save_report"]], "centralized_nlp_package.data_processing.spark_utils": [[3, 1, 1, "", "check_spark_dataframe_for_records"], [3, 1, 1, "", "convert_columns_to_timestamp"], [3, 1, 1, "", "define_structure"], [3, 1, 1, "", "equivalent_type"], [3, 1, 1, "", "get_default_dtype_mapping"], [3, 1, 1, "", "initialize_spark_session"], [3, 1, 1, "", "pandas_to_spark"], [3, 1, 1, "", "sparkdf_apply_transformations"]], "centralized_nlp_package.embedding": [[4, 0, 0, "-", "embedding_utils"], [4, 0, 0, "-", "word2vec_model"]], "centralized_nlp_package.embedding.embedding_utils": [[4, 1, 1, "", "average_token_embeddings"]], "centralized_nlp_package.embedding.word2vec_model": [[4, 1, 1, "", "save_word2vec_model"], [4, 1, 1, "", "train_word2vec_model"]], "centralized_nlp_package.model_utils": [[5, 0, 0, "-", "model_selector"], [5, 0, 0, "-", "models"]], "centralized_nlp_package.model_utils.model_selector": [[5, 1, 1, "", "get_best_models_by_param"], [5, 1, 1, "", "get_best_models_by_tag"], [5, 1, 1, "", "list_available_models"]], "centralized_nlp_package.model_utils.models": [[5, 2, 1, "", "DeBERTaModel"], [5, 2, 1, "", "FinBERTModel"], [5, 1, 1, "", "get_model"]], "centralized_nlp_package.model_utils.models.DeBERTaModel": [[5, 3, 1, "", "evaluate"], [5, 3, 1, "", "train"]], "centralized_nlp_package.model_utils.models.FinBERTModel": [[5, 3, 1, "", "train"]], "centralized_nlp_package.nli_utils": [[6, 0, 0, "-", "arguments"], [6, 0, 0, "-", "metrics"], [6, 0, 0, "-", "nli_inference"], [6, 0, 0, "-", "run_glue"]], "centralized_nlp_package.nli_utils.arguments": [[6, 2, 1, "", "DataTrainingArguments"], [6, 2, 1, "", "ModelArguments"]], "centralized_nlp_package.nli_utils.metrics": [[6, 1, 1, "", "compute_metrics"], [6, 1, 1, "", "evaluate_nli_models_from_path"], [6, 1, 1, "", "evaluate_nli_models_mlflow"], [6, 1, 1, "", "get_compute_metrics"], [6, 1, 1, "", "get_nli_model_metrics"]], "centralized_nlp_package.nli_utils.nli_inference": [[6, 1, 1, "", "initialize_nli_infer_pipeline"]], "centralized_nlp_package.nli_utils.run_glue": [[6, 1, 1, "", "run_finetune"], [6, 1, 1, "", "run_glue"]], "centralized_nlp_package.text_processing": [[7, 0, 0, "-", "text_analysis"], [7, 0, 0, "-", "text_preprocessing"], [7, 0, 0, "-", "text_utils"]], "centralized_nlp_package.text_processing.text_analysis": [[7, 1, 1, "", "calculate_net_score"], [7, 1, 1, "", "calculate_polarity_score"], [7, 1, 1, "", "calculate_sentence_score"], [7, 1, 1, "", "check_negation"], [7, 1, 1, "", "fog_analysis_per_section"], [7, 1, 1, "", "fog_analysis_per_sentence"], [7, 1, 1, "", "generate_match_count"], [7, 1, 1, "", "generate_sentence_relevance_score"], [7, 1, 1, "", "generate_topic_statistics"], [7, 1, 1, "", "get_match_set"], [7, 1, 1, "", "is_complex"], [7, 1, 1, "", "load_word_set"], [7, 1, 1, "", "match_count"], [7, 1, 1, "", "merge_counts"], [7, 1, 1, "", "polarity_score_per_section"], [7, 1, 1, "", "polarity_score_per_sentence"], [7, 1, 1, "", "tone_count_with_negation_check"], [7, 1, 1, "", "tone_count_with_negation_check_per_sentence"]], "centralized_nlp_package.text_processing.text_preprocessing": [[7, 1, 1, "", "clean_text"], [7, 1, 1, "", "initialize_spacy"], [7, 1, 1, "", "preprocess_text"], [7, 1, 1, "", "preprocess_text_list"], [7, 1, 1, "", "remove_unwanted_phrases_and_validate"], [7, 1, 1, "", "tokenize_and_lemmatize_text"], [7, 1, 1, "", "tokenize_matched_words"]], "centralized_nlp_package.text_processing.text_utils": [[7, 1, 1, "", "combine_sentiment_scores"], [7, 1, 1, "", "expand_contractions"], [7, 1, 1, "", "generate_ngrams"], [7, 1, 1, "", "load_set_from_txt"], [7, 1, 1, "", "load_syllable_counts"], [7, 1, 1, "", "tokenize_text"], [7, 1, 1, "", "validate_and_format_text"]], "centralized_nlp_package.utils": [[8, 0, 0, "-", "helper"]], "centralized_nlp_package.utils.helper": [[8, 1, 1, "", "determine_environment"], [8, 1, 1, "", "load_config_from_file"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"]}, "titleterms": {"welcom": 0, "central": 0, "nlp": 0, "packag": 0, "": 0, "document": 0, "content": 0, "indic": 0, "tabl": 0, "common": 1, "util": [1, 5, 6, 8], "data": [2, 3], "access": 2, "process": [3, 7], "embed": 4, "model": 5, "nli": 6, "text": 7}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1, "sphinx": 58}, "alltitles": {"Welcome to Centralized NLP Package\u2019s documentation!": [[0, "welcome-to-centralized-nlp-package-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Common Utils": [[1, "module-centralized_nlp_package.common_utils.date_utils"]], "Data Access": [[2, "module-centralized_nlp_package.data_access.snowflake_utils"]], "Data Processing": [[3, "module-centralized_nlp_package.data_processing.dataframe_utils"]], "Embedding": [[4, "module-centralized_nlp_package.embedding.embedding_utils"]], "Model Utils": [[5, "model-utils"]], "NLI utilities": [[6, "module-centralized_nlp_package.nli_utils.arguments"]], "Text processing": [[7, "module-centralized_nlp_package.text_processing.text_preprocessing"]], "Utilities": [[8, "module-centralized_nlp_package.utils.helper"]]}, "indexentries": {"centralized_nlp_package.common_utils.date_utils": [[1, "module-centralized_nlp_package.common_utils.date_utils"]], "centralized_nlp_package.common_utils.file_utils": [[1, "module-centralized_nlp_package.common_utils.file_utils"]], "centralized_nlp_package.common_utils.string_utils": [[1, "module-centralized_nlp_package.common_utils.string_utils"]], "format_date() (in module centralized_nlp_package.common_utils.date_utils)": [[1, "centralized_nlp_package.common_utils.date_utils.format_date"]], "format_string_template() (in module centralized_nlp_package.common_utils.string_utils)": [[1, "centralized_nlp_package.common_utils.string_utils.format_string_template"]], "get_current_date_str() (in module centralized_nlp_package.common_utils.date_utils)": [[1, "centralized_nlp_package.common_utils.date_utils.get_current_date_str"]], "get_date_range() (in module centralized_nlp_package.common_utils.date_utils)": [[1, "centralized_nlp_package.common_utils.date_utils.get_date_range"]], "load_content_from_txt() (in module centralized_nlp_package.common_utils.file_utils)": [[1, "centralized_nlp_package.common_utils.file_utils.load_content_from_txt"]], "module": [[1, "module-centralized_nlp_package.common_utils.date_utils"], [1, "module-centralized_nlp_package.common_utils.file_utils"], [1, "module-centralized_nlp_package.common_utils.string_utils"], [2, "module-centralized_nlp_package.data_access.snowflake_utils"], [3, "module-centralized_nlp_package.data_processing.dask_utils"], [3, "module-centralized_nlp_package.data_processing.dataframe_utils"], [3, "module-centralized_nlp_package.data_processing.spark_utils"], [4, "module-centralized_nlp_package.embedding.embedding_utils"], [4, "module-centralized_nlp_package.embedding.word2vec_model"], [5, "module-centralized_nlp_package.model_utils.model_selector"], [5, "module-centralized_nlp_package.model_utils.models"], [6, "module-centralized_nlp_package.nli_utils.arguments"], [6, "module-centralized_nlp_package.nli_utils.metrics"], [6, "module-centralized_nlp_package.nli_utils.nli_inference"], [6, "module-centralized_nlp_package.nli_utils.run_glue"], [7, "module-centralized_nlp_package.text_processing.text_analysis"], [7, "module-centralized_nlp_package.text_processing.text_preprocessing"], [7, "module-centralized_nlp_package.text_processing.text_utils"], [8, "module-centralized_nlp_package.utils.helper"]], "query_constructor() (in module centralized_nlp_package.common_utils.string_utils)": [[1, "centralized_nlp_package.common_utils.string_utils.query_constructor"]], "centralized_nlp_package.data_access.snowflake_utils": [[2, "module-centralized_nlp_package.data_access.snowflake_utils"]], "execute_truncate_or_merge_query() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.execute_truncate_or_merge_query"]], "read_from_snowflake() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.read_from_snowflake"]], "write_dataframe_to_snowflake() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.write_dataframe_to_snowflake"]], "centralized_nlp_package.data_processing.dask_utils": [[3, "module-centralized_nlp_package.data_processing.dask_utils"]], "centralized_nlp_package.data_processing.dataframe_utils": [[3, "module-centralized_nlp_package.data_processing.dataframe_utils"]], "centralized_nlp_package.data_processing.spark_utils": [[3, "module-centralized_nlp_package.data_processing.spark_utils"]], "check_pd_dataframe_for_records() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.check_pd_dataframe_for_records"]], "check_spark_dataframe_for_records() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.check_spark_dataframe_for_records"]], "concatenate_and_reset_index() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.concatenate_and_reset_index"]], "convert_columns_to_timestamp() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.convert_columns_to_timestamp"]], "dask_compute_with_progress() (in module centralized_nlp_package.data_processing.dask_utils)": [[3, "centralized_nlp_package.data_processing.dask_utils.dask_compute_with_progress"]], "define_structure() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.define_structure"]], "df_apply_transformations() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.df_apply_transformations"]], "df_remove_rows_with_keywords() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.df_remove_rows_with_keywords"]], "equivalent_type() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.equivalent_type"]], "get_default_dtype_mapping() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.get_default_dtype_mapping"]], "initialize_dask_client() (in module centralized_nlp_package.data_processing.dask_utils)": [[3, "centralized_nlp_package.data_processing.dask_utils.initialize_dask_client"]], "initialize_spark_session() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.initialize_spark_session"]], "pandas_to_spark() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.pandas_to_spark"]], "save_report() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.save_report"]], "sparkdf_apply_transformations() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.sparkdf_apply_transformations"]], "average_token_embeddings() (in module centralized_nlp_package.embedding.embedding_utils)": [[4, "centralized_nlp_package.embedding.embedding_utils.average_token_embeddings"]], "centralized_nlp_package.embedding.embedding_utils": [[4, "module-centralized_nlp_package.embedding.embedding_utils"]], "centralized_nlp_package.embedding.word2vec_model": [[4, "module-centralized_nlp_package.embedding.word2vec_model"]], "save_word2vec_model() (in module centralized_nlp_package.embedding.word2vec_model)": [[4, "centralized_nlp_package.embedding.word2vec_model.save_word2vec_model"]], "train_word2vec_model() (in module centralized_nlp_package.embedding.word2vec_model)": [[4, "centralized_nlp_package.embedding.word2vec_model.train_word2vec_model"]], "debertamodel (class in centralized_nlp_package.model_utils.models)": [[5, "centralized_nlp_package.model_utils.models.DeBERTaModel"]], "finbertmodel (class in centralized_nlp_package.model_utils.models)": [[5, "centralized_nlp_package.model_utils.models.FinBERTModel"]], "centralized_nlp_package.model_utils.model_selector": [[5, "module-centralized_nlp_package.model_utils.model_selector"]], "centralized_nlp_package.model_utils.models": [[5, "module-centralized_nlp_package.model_utils.models"]], "evaluate() (centralized_nlp_package.model_utils.models.debertamodel method)": [[5, "centralized_nlp_package.model_utils.models.DeBERTaModel.evaluate"]], "get_best_models_by_param() (in module centralized_nlp_package.model_utils.model_selector)": [[5, "centralized_nlp_package.model_utils.model_selector.get_best_models_by_param"]], "get_best_models_by_tag() (in module centralized_nlp_package.model_utils.model_selector)": [[5, "centralized_nlp_package.model_utils.model_selector.get_best_models_by_tag"]], "get_model() (in module centralized_nlp_package.model_utils.models)": [[5, "centralized_nlp_package.model_utils.models.get_model"]], "list_available_models() (in module centralized_nlp_package.model_utils.model_selector)": [[5, "centralized_nlp_package.model_utils.model_selector.list_available_models"]], "train() (centralized_nlp_package.model_utils.models.debertamodel method)": [[5, "centralized_nlp_package.model_utils.models.DeBERTaModel.train"]], "train() (centralized_nlp_package.model_utils.models.finbertmodel method)": [[5, "centralized_nlp_package.model_utils.models.FinBERTModel.train"]], "datatrainingarguments (class in centralized_nlp_package.nli_utils.arguments)": [[6, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments"]], "modelarguments (class in centralized_nlp_package.nli_utils.arguments)": [[6, "centralized_nlp_package.nli_utils.arguments.ModelArguments"]], "centralized_nlp_package.nli_utils.arguments": [[6, "module-centralized_nlp_package.nli_utils.arguments"]], "centralized_nlp_package.nli_utils.metrics": [[6, "module-centralized_nlp_package.nli_utils.metrics"]], "centralized_nlp_package.nli_utils.nli_inference": [[6, "module-centralized_nlp_package.nli_utils.nli_inference"]], "centralized_nlp_package.nli_utils.run_glue": [[6, "module-centralized_nlp_package.nli_utils.run_glue"]], "compute_metrics() (in module centralized_nlp_package.nli_utils.metrics)": [[6, "centralized_nlp_package.nli_utils.metrics.compute_metrics"]], "evaluate_nli_models_from_path() (in module centralized_nlp_package.nli_utils.metrics)": [[6, "centralized_nlp_package.nli_utils.metrics.evaluate_nli_models_from_path"]], "evaluate_nli_models_mlflow() (in module centralized_nlp_package.nli_utils.metrics)": [[6, "centralized_nlp_package.nli_utils.metrics.evaluate_nli_models_mlflow"]], "get_compute_metrics() (in module centralized_nlp_package.nli_utils.metrics)": [[6, "centralized_nlp_package.nli_utils.metrics.get_compute_metrics"]], "get_nli_model_metrics() (in module centralized_nlp_package.nli_utils.metrics)": [[6, "centralized_nlp_package.nli_utils.metrics.get_nli_model_metrics"]], "initialize_nli_infer_pipeline() (in module centralized_nlp_package.nli_utils.nli_inference)": [[6, "centralized_nlp_package.nli_utils.nli_inference.initialize_nli_infer_pipeline"]], "run_finetune() (in module centralized_nlp_package.nli_utils.run_glue)": [[6, "centralized_nlp_package.nli_utils.run_glue.run_finetune"]], "run_glue() (in module centralized_nlp_package.nli_utils.run_glue)": [[6, "centralized_nlp_package.nli_utils.run_glue.run_glue"]], "calculate_net_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.calculate_net_score"]], "calculate_polarity_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.calculate_polarity_score"]], "calculate_sentence_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.calculate_sentence_score"]], "centralized_nlp_package.text_processing.text_analysis": [[7, "module-centralized_nlp_package.text_processing.text_analysis"]], "centralized_nlp_package.text_processing.text_preprocessing": [[7, "module-centralized_nlp_package.text_processing.text_preprocessing"]], "centralized_nlp_package.text_processing.text_utils": [[7, "module-centralized_nlp_package.text_processing.text_utils"]], "check_negation() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.check_negation"]], "clean_text() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[7, "centralized_nlp_package.text_processing.text_preprocessing.clean_text"]], "combine_sentiment_scores() (in module centralized_nlp_package.text_processing.text_utils)": [[7, "centralized_nlp_package.text_processing.text_utils.combine_sentiment_scores"]], "expand_contractions() (in module centralized_nlp_package.text_processing.text_utils)": [[7, "centralized_nlp_package.text_processing.text_utils.expand_contractions"]], "fog_analysis_per_section() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.fog_analysis_per_section"]], "fog_analysis_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.fog_analysis_per_sentence"]], "generate_match_count() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.generate_match_count"]], "generate_ngrams() (in module centralized_nlp_package.text_processing.text_utils)": [[7, "centralized_nlp_package.text_processing.text_utils.generate_ngrams"]], "generate_sentence_relevance_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.generate_sentence_relevance_score"]], "generate_topic_statistics() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.generate_topic_statistics"]], "get_match_set() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.get_match_set"]], "initialize_spacy() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[7, "centralized_nlp_package.text_processing.text_preprocessing.initialize_spacy"]], "is_complex() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.is_complex"]], "load_set_from_txt() (in module centralized_nlp_package.text_processing.text_utils)": [[7, "centralized_nlp_package.text_processing.text_utils.load_set_from_txt"]], "load_syllable_counts() (in module centralized_nlp_package.text_processing.text_utils)": [[7, "centralized_nlp_package.text_processing.text_utils.load_syllable_counts"]], "load_word_set() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.load_word_set"]], "match_count() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.match_count"]], "merge_counts() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.merge_counts"]], "polarity_score_per_section() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.polarity_score_per_section"]], "polarity_score_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.polarity_score_per_sentence"]], "preprocess_text() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[7, "centralized_nlp_package.text_processing.text_preprocessing.preprocess_text"]], "preprocess_text_list() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[7, "centralized_nlp_package.text_processing.text_preprocessing.preprocess_text_list"]], "remove_unwanted_phrases_and_validate() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[7, "centralized_nlp_package.text_processing.text_preprocessing.remove_unwanted_phrases_and_validate"]], "tokenize_and_lemmatize_text() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[7, "centralized_nlp_package.text_processing.text_preprocessing.tokenize_and_lemmatize_text"]], "tokenize_matched_words() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[7, "centralized_nlp_package.text_processing.text_preprocessing.tokenize_matched_words"]], "tokenize_text() (in module centralized_nlp_package.text_processing.text_utils)": [[7, "centralized_nlp_package.text_processing.text_utils.tokenize_text"]], "tone_count_with_negation_check() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.tone_count_with_negation_check"]], "tone_count_with_negation_check_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[7, "centralized_nlp_package.text_processing.text_analysis.tone_count_with_negation_check_per_sentence"]], "validate_and_format_text() (in module centralized_nlp_package.text_processing.text_utils)": [[7, "centralized_nlp_package.text_processing.text_utils.validate_and_format_text"]], "centralized_nlp_package.utils.helper": [[8, "module-centralized_nlp_package.utils.helper"]], "determine_environment() (in module centralized_nlp_package.utils.helper)": [[8, "centralized_nlp_package.utils.helper.determine_environment"]], "load_config_from_file() (in module centralized_nlp_package.utils.helper)": [[8, "centralized_nlp_package.utils.helper.load_config_from_file"]]}})