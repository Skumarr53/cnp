Search.setIndex({"docnames": ["index", "source/common_utils", "source/data_access", "source/data_processing", "source/embedding", "source/nli_utils", "source/text_processing", "source/utils"], "filenames": ["index.rst", "source/common_utils.rst", "source/data_access.rst", "source/data_processing.rst", "source/embedding.rst", "source/nli_utils.rst", "source/text_processing.rst", "source/utils.rst"], "titles": ["Welcome to Centralized NLP Package\u2019s documentation!", "Common Utils", "Data Access", "Data Processing", "Embedding", "NLI utilities", "Text processing", "Utilities"], "terms": {"data": [0, 1, 5, 6], "access": 0, "execute_truncate_or_merge_queri": [0, 2], "get_snowflake_connection_opt": [0, 2], "read_from_snowflak": [0, 2], "retrieve_snowflake_private_kei": [0, 2], "singleton": [0, 2], "with_spark_sess": [0, 2], "write_dataframe_to_snowflak": [0, 2], "embed": 0, "average_token_embed": [0, 4], "save_word2vec_model": [0, 4], "train_word2vec_model": [0, 4], "text": [0, 1, 3], "process": [0, 2, 5], "clean_text": [0, 6], "initialize_spaci": [0, 6], "preprocess_text": [0, 6], "preprocess_text_list": [0, 6], "remove_unwanted_phrases_and_valid": [0, 6], "tokenize_and_lemmatize_text": [0, 6], "tokenize_matched_word": [0, 6], "combine_sentiment_scor": [0, 6], "expand_contract": [0, 6], "generate_ngram": [0, 6], "load_set_from_txt": [0, 6], "load_syllable_count": [0, 6], "tokenize_text": [0, 6], "validate_and_format_text": [0, 6], "calculate_polarity_scor": [0, 6], "calculate_sentence_scor": [0, 6], "check_neg": [0, 6], "fog_analysis_per_sect": [0, 6], "fog_analysis_per_sent": [0, 6], "generate_match_count": [0, 6], "generate_sentence_relevance_scor": [0, 6], "generate_topic_statist": [0, 6], "get_match_set": [0, 6], "is_complex": [0, 6], "load_word_set": [0, 6], "match_count": [0, 6], "merge_count": [0, 6], "netscor": [0, 6], "polarity_score_per_sect": [0, 6], "polarity_score_per_sent": [0, 6], "tone_count_with_negation_check": [0, 6], "tone_count_with_negation_check_per_sent": [0, 6], "util": [0, 2], "determine_environ": [0, 7], "load_config_from_fil": [0, 7], "common": 0, "format_d": [0, 1], "get_date_rang": [0, 1], "load_content_from_txt": [0, 1], "format_string_templ": [0, 1], "query_constructor": [0, 1], "check_pd_dataframe_for_record": [0, 3], "concatenate_and_reset_index": [0, 3], "df_apply_transform": [0, 3], "df_remove_rows_with_keyword": [0, 3], "save_report": [0, 3], "dask_compute_with_progress": [0, 3], "initialize_dask_cli": [0, 3], "check_spark_dataframe_for_record": [0, 3], "convert_columns_to_timestamp": [0, 3], "create_spark_udf": [0, 3], "define_structur": [0, 3], "equivalent_typ": [0, 3], "get_default_dtype_map": [0, 3], "initialize_spark_sess": [0, 3], "keyword_to_datatyp": [0, 3], "pandas_to_spark": [0, 3], "sparkdf_apply_transform": [0, 3], "nli": 0, "datatrainingargu": [0, 5], "modelargu": [0, 5], "prepare_dataset": [0, 5], "preprocess_dataset": [0, 5], "get_compute_metr": [0, 5], "evalu": [0, 5, 6], "initialize_train": [0, 5], "predict": [0, 5], "setup_log": [0, 5], "train": [0, 4, 5], "main": [0, 5], "run_glu": [0, 5], "initialize_nli_infer_pipelin": [0, 5], "index": [0, 3, 6], "modul": 0, "search": 0, "page": 0, "centralized_nlp_packag": [1, 2, 3, 4, 5, 6, 7], "common_util": 1, "date_util": 1, "date": [1, 3], "sourc": [1, 2, 3, 4, 5, 6, 7], "format": [1, 2, 3, 4, 6], "datetim": [1, 3], "object": [1, 3, 5, 6, 7], "string": [1, 3, 6], "yyyi": [1, 3], "mm": [1, 3], "dd": [1, 3], "paramet": [1, 2, 3, 4, 5, 6, 7], "The": [1, 2, 3, 4, 5, 6, 7], "return": [1, 2, 3, 4, 5, 6, 7], "type": [1, 2, 3, 4, 5, 6, 7], "str": [1, 2, 3, 4, 5, 6, 7], "exampl": [1, 2, 3, 4, 5, 6, 7], "from": [1, 2, 3, 4, 5, 6], "import": [1, 2, 3, 4, 5, 6], "2023": [1, 3], "9": [1, 3, 5], "15": [1, 4], "09": 1, "years_back": 1, "0": [1, 3, 4, 5, 6], "months_back": 1, "calcul": [1, 6], "rang": 1, "base": [1, 3, 5, 6, 7], "current": [1, 6], "minu": 1, "specifi": [1, 2, 3, 4, 6, 7], "year": 1, "month": 1, "int": [1, 3, 5, 6], "option": [1, 2, 3, 4, 5, 6, 7], "number": [1, 3, 5, 6], "go": [1, 6], "back": [1, 5], "default": [1, 2, 3, 4, 5, 6], "A": [1, 2, 3, 4, 5, 6], "tupl": [1, 3, 6], "contain": [1, 2, 3, 5, 6], "start": 1, "end": 1, "start_dat": 1, "end_dat": 1, "1": [1, 3, 4, 5, 6], "2": [1, 3, 5, 6], "exmapl": 1, "i": [1, 2, 3, 5, 6, 7], "10": [1, 3, 4, 6], "01": [1, 3, 5], "print": [1, 3, 4, 6], "2022": 1, "08": 1, "file_util": 1, "file_path": [1, 6, 7], "read": [1, 6], "entir": 1, "content": [1, 6, 7], "file": [1, 3, 4, 5, 6, 7], "given": [1, 2, 3, 6], "path": [1, 3, 4, 5, 6, 7], "rais": [1, 2, 3, 6, 7], "filesnotloadedexcept": [1, 6], "If": [1, 2, 3, 6, 7], "found": [1, 3, 6], "sampl": [1, 3, 5, 6], "txt": [1, 6], "thi": [1, 2, 3, 4, 6, 7], "string_util": 1, "templat": 1, "kwarg": [1, 4], "construct": [1, 2], "replac": 1, "placehold": 1, "provid": [1, 2, 3, 4, 5, 6, 7], "keyword": [1, 3, 4], "argument": [1, 3, 4, 5, 7], "form": 1, "kei": [1, 2, 3, 6], "variabl": 1, "valueerror": [1, 3, 7], "ani": [1, 3, 4, 6], "doe": [1, 3], "have": 1, "correspond": [1, 3], "hello": [1, 3, 4, 6], "name": [1, 2, 3, 5, 6, 7], "todai": 1, "dai": 1, "alic": [1, 3], "mondai": 1, "query_input": 1, "queri": [1, 2], "load": [1, 5, 6, 7], "us": [1, 2, 3, 4, 5, 6, 7], "union": [1, 3, 6], "o": [1, 3], "pathlik": 1, "itself": 1, "substitut": 1, "do": [1, 6], "match": [1, 6], "exist": [1, 2, 3, 4], "select": [1, 2], "user": [1, 2, 3, 6, 7], "where": [1, 2, 3, 6], "signup_d": 1, "AND": 1, "12": [1, 3, 6], "31": 1, "data_access": [2, 3], "snowflake_util": 2, "databas": 2, "schema": [2, 3], "execut": [2, 5], "truncat": [2, 3, 5], "merg": [2, 6], "sql": [2, 3], "snowflak": 2, "function": [2, 3, 4, 5, 6, 7], "run": [2, 5], "either": [2, 4], "against": [2, 6], "establish": 2, "spark": [2, 3], "session": [2, 3], "It": 2, "ensur": [2, 4, 6], "within": [2, 6], "correct": 2, "context": 2, "typic": 2, "statement": 2, "target": 2, "confirm": 2, "messag": [2, 3], "indic": [2, 6], "success": 2, "complet": 2, "oper": [2, 3], "except": [2, 3, 6], "an": [2, 3, 4, 5, 6, 7], "error": [2, 6, 7], "tabl": 2, "analytics_db": 2, "public": 2, "sales_data": 2, "eds_prod": 2, "quant": 2, "env": 2, "none": [2, 3, 4, 5, 6, 7], "dictionari": [2, 3, 6], "connect": 2, "includ": [2, 6, 7], "account": [2, 6], "url": 2, "credenti": 2, "privat": 2, "timezon": 2, "role": 2, "dict": [2, 3, 5, 6], "snowflake_opt": 2, "result": [2, 3, 6], "datafram": [2, 3, 6], "set": [2, 3, 6], "appropri": 2, "df": [2, 3, 6], "show": [2, 3], "config": [2, 5, 6, 7], "retriev": 2, "azur": 2, "vault": 2, "akv": 2, "fetch": 2, "encrypt": 2, "password": 2, "decrypt": 2, "authent": 2, "pem": 2, "suitabl": 2, "private_kei": 2, "cl": 2, "decor": 2, "make": 2, "class": [2, 5], "onli": [2, 6], "one": [2, 3], "instanc": [2, 3, 5], "func": [2, 3], "initi": [2, 3, 4, 5, 6, 7], "befor": 2, "table_nam": 2, "mode": 2, "append": 2, "write": 2, "allow": [2, 5, 6], "correctli": 2, "direct": 2, "intend": 2, "locat": 2, "written": 2, "behavior": 2, "alreadi": 2, "ar": [2, 3, 4, 5, 6], "overwrit": [2, 3, 5], "new": [2, 3, 6], "throw": 2, "ignor": 2, "data_process": 3, "dataframe_util": 3, "datetime_col": 3, "parsed_datetime_eastern_tz": 3, "exit_on_empti": 3, "true": [3, 5, 6], "exit_method": 3, "both": [3, 6], "check": [3, 6], "non": [3, 6], "empti": [3, 6], "relev": [3, 6], "inform": 3, "exit": 3, "program": 3, "pd": [3, 6], "column": [3, 6], "determin": [3, 6, 7], "span": 3, "bool": [3, 4, 5, 6], "whether": [3, 4, 5, 6], "method": 3, "dbutil": 3, "drop_column": 3, "concaten": 3, "multipl": [3, 6], "panda": [3, 6], "reset": 3, "drop": 3, "old": 3, "perform": [3, 6], "follow": 3, "step": 3, "along": 3, "row": 3, "list": [3, 4, 6], "after": [3, 5], "input": [3, 5, 6], "keyerror": 3, "rdf": 3, "b": 3, "3": [3, 5, 6], "4": [3, 6], "cdf": 3, "5": [3, 4, 5, 6], "6": [3, 6], "7": [3, 6], "8": [3, 5, 6], "sdf": 3, "11": 3, "concatdf": 3, "transform": [3, 5], "appli": [3, 6], "each": [3, 6], "should": 3, "new_column": 3, "creat": 3, "columns_to_us": 3, "": [3, 4, 6], "callabl": [3, 5], "invalid": [3, 6], "re": 3, "occur": 3, "dure": [3, 6], "log": [3, 5], "def": 3, "concat_column": 3, "f": 3, "_": 3, "col1": 3, "col2": 3, "c": 3, "d": 3, "col3": 3, "lambda": 3, "transformed_df": 3, "a_c": 3, "b_d": 3, "column_nam": 3, "filter": [3, 6], "remov": [3, 6], "out": 3, "comment": 3, "good": [3, 6], "product": [3, 6], "bad": [3, 6], "servic": [3, 6], "averag": [3, 4, 6], "experi": 3, "filtered_df": 3, "save": [3, 4], "report": 3, "csv": [3, 5], "parquet": 3, "extens": 3, "infer": 3, "support": 3, "importerror": 3, "requir": [3, 6], "engin": 3, "instal": 3, "usag": [3, 5, 6], "save_report_modul": 3, "assum": 3, "py": 3, "bob": 3, "charli": 3, "score": [3, 6], "85": 3, "92": 3, "78": 3, "dask_util": 3, "dask_datafram": 3, "use_progress": 3, "comput": [3, 5], "dask": 3, "displai": 3, "progress": 3, "bar": 3, "e": [3, 4, 7], "g": [3, 4, 7], "read_csv": 3, "computed_df": 3, "head": [3, 5], "column1": 3, "column2": 3, "n_worker": 3, "32": 3, "threads_per_work": 3, "client": 3, "worker": [3, 4], "thread": 3, "per": [3, 6], "distribut": 3, "0x": 3, "spark_util": 3, "spark_df": 3, "record": 3, "present": 3, "minimum": [3, 6], "maximum": [3, 5, 6], "pars": 3, "count": [3, 6], "warn": 3, "notebook": 3, "valid": [3, 5, 6], "columns_format": 3, "convert": [3, 6], "timestamp": 3, "iter": [3, 6], "over": [3, 6], "to_timestamp": 3, "valu": [3, 6], "rubric": 3, "report_d": 3, "hh": 3, "ss": 3, "event_datetime_utc": 3, "fals": [3, 4, 5, 6], "suffix": [3, 6], "_t": 3, "pyspark": 3, "sparksess": 3, "builder": 3, "appnam": 3, "exampleapp": 3, "getorcr": 3, "00": 3, "02": 3, "13": 3, "30": 3, "45": 3, "createdatafram": 3, "columns_to_convert": 3, "converted_df": 3, "printschema": 3, "root": 3, "nullabl": 3, "return_type_kei": 3, "arr": 3, "defin": [3, 5, 6], "udf": 3, "python": 3, "arrai": [3, 4, 5], "userdefinedfunct": 3, "creation": 3, "fail": 3, "other": 3, "reason": 3, "pandas_dtyp": 3, "column_map": 3, "structfield": 3, "structtyp": 3, "dtype": [3, 4, 6], "map": 3, "identifi": [3, 5, 6], "datatyp": 3, "prioriti": 3, "app_nam": 3, "optimized_nli_infer": 3, "shuffle_partit": 3, "200": 3, "gpu_amount": 3, "task_gpu_amount": 3, "executor_memori": 3, "4g": 3, "driver_memori": 3, "2g": 3, "executor_cor": 3, "memory_overhead": 3, "512m": 3, "dynamic_alloc": 3, "configur": [3, 4, 5, 6, 7], "applic": 3, "partit": 3, "shuffl": 3, "float": [3, 5, 6], "amount": 3, "gpu": 3, "resourc": 3, "alloc": 3, "executor": 3, "task": [3, 5], "memori": 3, "driver": 3, "core": 3, "overhead": 3, "enabl": [3, 5], "dynam": 3, "pandas_df": 3, "column_type_map": 3, "customiz": 3, "its": [3, 6], "predefin": [3, 6], "stringtyp": 3, "int64": 3, "longtyp": 3, "int32": 3, "integertyp": 3, "float64": [3, 6], "doubletyp": 3, "float32": [3, 4], "floattyp": 3, "booleantyp": 3, "datetime64": 3, "n": [3, 6], "timestamptyp": 3, "timedelta": 3, "activ": 3, "filt_md": 3, "arr_str": 3, "stat": [3, 6], "map_str_int": 3, "custom": [3, 5, 6], "arraytyp": 3, "long": 3, "doubl": 3, "maptyp": 3, "issu": [3, 7], "convers": 3, "error_on_miss": 3, "seri": 3, "specif": [3, 5, 6], "overwritten": 3, "new_column_nam": 3, "input_column": 3, "transformation_funct": 3, "pass": 3, "take": [3, 7], "more": 3, "miss": 3, "skip": 3, "all": [3, 5], "typeerror": [3, 6], "structur": 3, "transformationexampl": 3, "world": [3, 4, 6], "text_column": 3, "date_column": 3, "to_upp": 3, "upper": 3, "els": [3, 6], "extract_year": 3, "date_str": 3, "split": 3, "to_upper_udf": 3, "extract_year_udf": 3, "text_upp": 3, "year_extract": 3, "combin": [3, 6], "concat": 3, "lit": 3, "apply_transform": 3, "output": [3, 5], "embedding_util": 4, "token": [4, 5, 6], "model": [4, 5, 6], "gener": [4, 6], "vector": 4, "unigram": [4, 6], "bigram": [4, 6], "word2vec": 4, "np": [4, 5], "ndarrai": 4, "gensim": 4, "king": 4, "queen": 4, "man": 4, "vector_s": 4, "100": 4, "min_count": 4, "epoch": 4, "unknown_token": 4, "shape": 4, "word2vec_model": 4, "directori": [4, 5, 7], "nativ": 4, "sentenc": [4, 6], "300": 4, "window": [4, 6], "16": [4, 5], "corpu": 4, "librari": [4, 5], "addit": [4, 6], "can": [4, 5, 6], "via": [4, 5], "machin": 4, "learn": [4, 5], "wv": 4, "0123": 4, "0456": 4, "0789": 4, "nli_util": 5, "task_nam": 5, "mnli": 5, "dataset_nam": 5, "dataset_config_nam": 5, "max_seq_length": 5, "128": 5, "overwrite_cach": 5, "pad_to_max_length": 5, "max_train_sampl": 5, "max_eval_sampl": 5, "max_predict_sampl": 5, "train_fil": 5, "validation_fil": 5, "test_fil": 5, "pertain": 5, "dataset": 5, "total": [5, 6], "sequenc": 5, "length": [5, 6], "cach": 5, "preprocess": [5, 6], "pad": 5, "debug": 5, "quicker": 5, "json": 5, "test": [5, 6], "model_name_or_path": 5, "dbf": 5, "mnt": 5, "access_work": 5, "uc25": 5, "huggingfac": 5, "deberta": 5, "v3": 5, "larg": 5, "zeroshot": 5, "v2": 5, "cache_dir": 5, "topic": [5, 6], "fine": 5, "tune": 5, "trained_rd_deberta": 5, "v2_v3": 5, "learning_r": 5, "2e": 5, "05": 5, "weight_decai": 5, "per_device_train_batch_s": 5, "per_device_eval_batch_s": 5, "pretrain": 5, "co": 5, "config_nam": 5, "same": 5, "model_nam": 5, "tokenizer_nam": 5, "store": 5, "download": 5, "use_fast_token": 5, "fast": 5, "model_revis": 5, "version": 5, "branch": 5, "tag": [5, 6], "commit": 5, "id": 5, "http": [5, 6], "bearer": 5, "author": 5, "remot": 5, "trust_remote_cod": 5, "hub": 5, "own": 5, "ignore_mismatched_s": 5, "whose": 5, "dimens": 5, "differ": 5, "rate": 5, "weight": [5, 6], "decai": 5, "optim": 5, "batch": 5, "size": 5, "data_arg": 5, "model_arg": 5, "relat": 5, "datasetdict": 5, "nli_finetun": 5, "bert": 5, "uncas": 5, "raw_dataset": 5, "raw": 5, "pretrainedtoken": 5, "autotoken": 5, "from_pretrain": 5, "tokenized_dataset": 5, "metric": 5, "is_regress": 5, "regress": 5, "glue": 5, "evalpredict": 5, "compute_metr": 5, "pred": 5, "label": [5, 6], "eval_pr": 5, "label_id": 5, "nli_train": 5, "trainer": 5, "training_arg": 5, "train_dataset": 5, "eval_dataset": 5, "data_col": 5, "hug": 5, "face": 5, "automodelforsequenceclassif": 5, "trainingargu": 5, "collat": 5, "orchestr": 5, "prepar": 5, "output_dir": 5, "do_train": 5, "do_ev": 5, "num_train_epoch": 5, "report_to": 5, "nli_infer": 5, "model_path": 5, "enable_quant": 5, "text_process": 6, "text_preprocess": 6, "clean": 6, "expand": 6, "contract": 6, "unwant": 6, "charact": 6, "normal": 6, "space": 6, "t": 6, "parti": 6, "cannot": [6, 7], "en_core_web_sm": 6, "max_length": 6, "1000000000": 6, "exclude_stop_word": 6, "bottom": 6, "top": 6, "call": 6, "spaci": 6, "languag": 6, "adjust": 6, "stop": 6, "word": 6, "exclud": 6, "en_core_web_md": 6, "en_core_web_lg": 6, "etc": 6, "higher": 6, "larger": 6, "what": 6, "consid": 6, "insignific": 6, "readi": 6, "nlp": 6, "2000000000": 6, "doc": 6, "text_input": 6, "cleaned_text": 6, "believ": 6, "text_list": 6, "love": 6, "hi": 6, "tokens_list": 6, "min_word_length": 6, "cleanup_phras": 6, "greeting_phras": 6, "greet": 6, "phrase": 6, "thank": 6, "you": 6, "earn": 6, "releas": 6, "confer": 6, "morn": 6, "afternoon": 6, "even": 6, "For": 6, "unexpect": 6, "pos_exclud": 6, "ent_type_exclud": 6, "lemmat": 6, "document": 6, "punctuat": 6, "part": 6, "speech": 6, "entiti": 6, "avail": 6, "po": 6, "refer": 6, "io": 6, "linguist": 6, "featur": 6, "www": 6, "restack": 6, "p": 6, "recognit": 6, "answer": 6, "cat": 6, "ai": 6, "am": 6, "appl": 6, "look": 6, "bui": 6, "u": 6, "k": 6, "startup": 6, "billion": 6, "verb": 6, "org": 6, "priorit": 6, "proper": 6, "noun": 6, "capit": 6, "john": 6, "york": 6, "text_util": 6, "positive_count": 6, "negative_count": 6, "two": 6, "sentiment": 6, "singl": 6, "posit": 6, "neg": 6, "zero": 6, "25": 6, "contraction_map": 6, "m": 6, "input_list": 6, "gram": 6, "yield": 6, "code": 6, "is_low": 6, "line": 6, "lowercas": 6, "word_set": 6, "positive_word": 6, "happi": 6, "joy": 6, "delight": 6, "syllabl": 6, "ha": 6, "syllable_count": 6, "beauti": 6, "spacy_token": 6, "join": 6, "them": 6, "strip": 6, "text_analysi": 6, "input_word": 6, "negative_word": 6, "negation_word": 6, "polar": 6, "negat": 6, "sum": 6, "legaci": 6, "like": 6, "hate": 6, "dislik": 6, "never": 6, "apply_weight": 6, "preced": 6, "three": 6, "otherwis": 6, "fog": 6, "readabl": 6, "analyz": 6, "complex": 6, "simpl": 6, "unnecessarili": 6, "verbos": 6, "fog_scor": 6, "14": 6, "word_set_dict": 6, "updat": 6, "section1": 6, "updated_df": 6, "matches_section1": 6, "sent_labels_section1": 6, "love_total_section1": 6, "love_sent_section1": 6, "statist": 6, "uni": 6, "bi": 6, "len_section1": 6, "raw_len_section1": 6, "bad_total_section1": 6, "love_stats_section1": 6, "bad_stats_section1": 6, "num_sents_section1": 6, "origin": 6, "veri": 6, "extrem": 6, "match_set": 6, "rule": 6, "syllable_dict": 6, "filenam": [6, 7], "artifact": 6, "len": 6, "1500": 6, "very_happi": 6, "net": 6, "occurr": 6, "helper": 7, "provided_env": 7, "environ": 7, "auto": 7, "detect": 7, "databrick": 7, "workspac": 7, "dev": 7, "stg": 7, "prod": 7, "hydra": 7, "full": 7, "extract": 7, "compos": 7, "yaml": 7, "dictconfig": 7, "runtimeerror": 7, "get_config": 7, "your": 7}, "objects": {"centralized_nlp_package.common_utils": [[1, 0, 0, "-", "date_utils"], [1, 0, 0, "-", "file_utils"], [1, 0, 0, "-", "string_utils"]], "centralized_nlp_package.common_utils.date_utils": [[1, 1, 1, "", "format_date"], [1, 1, 1, "", "get_date_range"]], "centralized_nlp_package.common_utils.file_utils": [[1, 1, 1, "", "load_content_from_txt"]], "centralized_nlp_package.common_utils.string_utils": [[1, 1, 1, "", "format_string_template"], [1, 1, 1, "", "query_constructor"]], "centralized_nlp_package.data_access": [[2, 0, 0, "-", "snowflake_utils"]], "centralized_nlp_package.data_access.snowflake_utils": [[2, 1, 1, "", "execute_truncate_or_merge_query"], [2, 1, 1, "", "get_snowflake_connection_options"], [2, 1, 1, "", "read_from_snowflake"], [2, 1, 1, "", "retrieve_snowflake_private_key"], [2, 1, 1, "", "singleton"], [2, 1, 1, "", "with_spark_session"], [2, 1, 1, "", "write_dataframe_to_snowflake"]], "centralized_nlp_package.data_processing": [[3, 0, 0, "-", "dask_utils"], [3, 0, 0, "-", "dataframe_utils"], [3, 0, 0, "-", "spark_utils"]], "centralized_nlp_package.data_processing.dask_utils": [[3, 1, 1, "", "dask_compute_with_progress"], [3, 1, 1, "", "initialize_dask_client"]], "centralized_nlp_package.data_processing.dataframe_utils": [[3, 1, 1, "", "check_pd_dataframe_for_records"], [3, 1, 1, "", "concatenate_and_reset_index"], [3, 1, 1, "", "df_apply_transformations"], [3, 1, 1, "", "df_remove_rows_with_keywords"], [3, 1, 1, "", "save_report"]], "centralized_nlp_package.data_processing.spark_utils": [[3, 1, 1, "", "check_spark_dataframe_for_records"], [3, 1, 1, "", "convert_columns_to_timestamp"], [3, 1, 1, "", "create_spark_udf"], [3, 1, 1, "", "define_structure"], [3, 1, 1, "", "equivalent_type"], [3, 1, 1, "", "get_default_dtype_mapping"], [3, 1, 1, "", "initialize_spark_session"], [3, 1, 1, "", "keyword_to_datatype"], [3, 1, 1, "", "pandas_to_spark"], [3, 1, 1, "", "sparkdf_apply_transformations"]], "centralized_nlp_package.embedding": [[4, 0, 0, "-", "embedding_utils"], [4, 0, 0, "-", "word2vec_model"]], "centralized_nlp_package.embedding.embedding_utils": [[4, 1, 1, "", "average_token_embeddings"]], "centralized_nlp_package.embedding.word2vec_model": [[4, 1, 1, "", "save_word2vec_model"], [4, 1, 1, "", "train_word2vec_model"]], "centralized_nlp_package.nli_utils": [[5, 0, 0, "-", "arguments"], [5, 0, 0, "-", "data"], [5, 0, 0, "-", "metrics"], [5, 0, 0, "-", "nli_inference"], [5, 0, 0, "-", "nli_trainer"], [5, 0, 0, "-", "run_glue"]], "centralized_nlp_package.nli_utils.arguments": [[5, 2, 1, "", "DataTrainingArguments"], [5, 2, 1, "", "ModelArguments"]], "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments": [[5, 3, 1, "id0", "dataset_config_name"], [5, 3, 1, "id1", "dataset_name"], [5, 3, 1, "id2", "max_eval_samples"], [5, 3, 1, "id3", "max_predict_samples"], [5, 3, 1, "id4", "max_seq_length"], [5, 3, 1, "id5", "max_train_samples"], [5, 3, 1, "id6", "overwrite_cache"], [5, 3, 1, "id7", "pad_to_max_length"], [5, 3, 1, "id8", "task_name"], [5, 3, 1, "id9", "test_file"], [5, 3, 1, "id10", "train_file"], [5, 3, 1, "id11", "validation_file"]], "centralized_nlp_package.nli_utils.arguments.ModelArguments": [[5, 3, 1, "id12", "cache_dir"], [5, 3, 1, "", "config_name"], [5, 3, 1, "", "ignore_mismatched_sizes"], [5, 3, 1, "id13", "learning_rate"], [5, 3, 1, "id14", "model_name_or_path"], [5, 3, 1, "", "model_revision"], [5, 3, 1, "id15", "per_device_eval_batch_size"], [5, 3, 1, "id16", "per_device_train_batch_size"], [5, 3, 1, "", "token"], [5, 3, 1, "", "tokenizer_name"], [5, 3, 1, "", "trust_remote_code"], [5, 3, 1, "", "use_fast_tokenizer"], [5, 3, 1, "id17", "weight_decay"]], "centralized_nlp_package.nli_utils.data": [[5, 1, 1, "", "prepare_datasets"], [5, 1, 1, "", "preprocess_datasets"]], "centralized_nlp_package.nli_utils.metrics": [[5, 1, 1, "", "get_compute_metrics"]], "centralized_nlp_package.nli_utils.nli_inference": [[5, 1, 1, "", "initialize_nli_infer_pipeline"]], "centralized_nlp_package.nli_utils.nli_trainer": [[5, 1, 1, "", "evaluate"], [5, 1, 1, "", "initialize_trainer"], [5, 1, 1, "", "predict"], [5, 1, 1, "", "setup_logging"], [5, 1, 1, "", "train"]], "centralized_nlp_package.nli_utils.run_glue": [[5, 1, 1, "", "main"], [5, 1, 1, "", "run_glue"]], "centralized_nlp_package.text_processing": [[6, 0, 0, "-", "text_analysis"], [6, 0, 0, "-", "text_preprocessing"], [6, 0, 0, "-", "text_utils"]], "centralized_nlp_package.text_processing.text_analysis": [[6, 1, 1, "", "calculate_polarity_score"], [6, 1, 1, "", "calculate_sentence_score"], [6, 1, 1, "", "check_negation"], [6, 1, 1, "", "fog_analysis_per_section"], [6, 1, 1, "", "fog_analysis_per_sentence"], [6, 1, 1, "", "generate_match_count"], [6, 1, 1, "", "generate_sentence_relevance_score"], [6, 1, 1, "", "generate_topic_statistics"], [6, 1, 1, "", "get_match_set"], [6, 1, 1, "", "is_complex"], [6, 1, 1, "", "load_word_set"], [6, 1, 1, "", "match_count"], [6, 1, 1, "", "merge_counts"], [6, 1, 1, "", "netscore"], [6, 1, 1, "", "polarity_score_per_section"], [6, 1, 1, "", "polarity_score_per_sentence"], [6, 1, 1, "", "tone_count_with_negation_check"], [6, 1, 1, "", "tone_count_with_negation_check_per_sentence"]], "centralized_nlp_package.text_processing.text_preprocessing": [[6, 1, 1, "", "clean_text"], [6, 1, 1, "", "initialize_spacy"], [6, 1, 1, "", "preprocess_text"], [6, 1, 1, "", "preprocess_text_list"], [6, 1, 1, "", "remove_unwanted_phrases_and_validate"], [6, 1, 1, "", "tokenize_and_lemmatize_text"], [6, 1, 1, "", "tokenize_matched_words"]], "centralized_nlp_package.text_processing.text_utils": [[6, 1, 1, "", "combine_sentiment_scores"], [6, 1, 1, "", "expand_contractions"], [6, 1, 1, "", "generate_ngrams"], [6, 1, 1, "", "load_set_from_txt"], [6, 1, 1, "", "load_syllable_counts"], [6, 1, 1, "", "tokenize_text"], [6, 1, 1, "", "validate_and_format_text"]], "centralized_nlp_package.utils": [[7, 0, 0, "-", "helper"]], "centralized_nlp_package.utils.helper": [[7, 1, 1, "", "determine_environment"], [7, 1, 1, "", "load_config_from_file"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:attribute"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "attribute", "Python attribute"]}, "titleterms": {"welcom": 0, "central": 0, "nlp": 0, "packag": 0, "": 0, "document": 0, "content": 0, "indic": 0, "tabl": 0, "common": 1, "util": [1, 5, 7], "data": [2, 3], "access": 2, "process": [3, 6], "embed": 4, "nli": 5, "text": 6}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1, "sphinx": 58}, "alltitles": {"Welcome to Centralized NLP Package\u2019s documentation!": [[0, "welcome-to-centralized-nlp-package-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "Common Utils": [[1, "module-centralized_nlp_package.common_utils.date_utils"]], "Data Access": [[2, "module-centralized_nlp_package.data_access.snowflake_utils"]], "Data Processing": [[3, "module-centralized_nlp_package.data_processing.dataframe_utils"]], "Embedding": [[4, "module-centralized_nlp_package.embedding.embedding_utils"]], "NLI utilities": [[5, "module-centralized_nlp_package.nli_utils.arguments"]], "Text processing": [[6, "module-centralized_nlp_package.text_processing.text_preprocessing"]], "Utilities": [[7, "module-centralized_nlp_package.utils.helper"]]}, "indexentries": {"centralized_nlp_package.common_utils.date_utils": [[1, "module-centralized_nlp_package.common_utils.date_utils"]], "centralized_nlp_package.common_utils.file_utils": [[1, "module-centralized_nlp_package.common_utils.file_utils"]], "centralized_nlp_package.common_utils.string_utils": [[1, "module-centralized_nlp_package.common_utils.string_utils"]], "format_date() (in module centralized_nlp_package.common_utils.date_utils)": [[1, "centralized_nlp_package.common_utils.date_utils.format_date"]], "format_string_template() (in module centralized_nlp_package.common_utils.string_utils)": [[1, "centralized_nlp_package.common_utils.string_utils.format_string_template"]], "get_date_range() (in module centralized_nlp_package.common_utils.date_utils)": [[1, "centralized_nlp_package.common_utils.date_utils.get_date_range"]], "load_content_from_txt() (in module centralized_nlp_package.common_utils.file_utils)": [[1, "centralized_nlp_package.common_utils.file_utils.load_content_from_txt"]], "module": [[1, "module-centralized_nlp_package.common_utils.date_utils"], [1, "module-centralized_nlp_package.common_utils.file_utils"], [1, "module-centralized_nlp_package.common_utils.string_utils"], [2, "module-centralized_nlp_package.data_access.snowflake_utils"], [3, "module-centralized_nlp_package.data_processing.dask_utils"], [3, "module-centralized_nlp_package.data_processing.dataframe_utils"], [3, "module-centralized_nlp_package.data_processing.spark_utils"], [4, "module-centralized_nlp_package.embedding.embedding_utils"], [4, "module-centralized_nlp_package.embedding.word2vec_model"], [5, "module-centralized_nlp_package.nli_utils.arguments"], [5, "module-centralized_nlp_package.nli_utils.data"], [5, "module-centralized_nlp_package.nli_utils.metrics"], [5, "module-centralized_nlp_package.nli_utils.nli_inference"], [5, "module-centralized_nlp_package.nli_utils.nli_trainer"], [5, "module-centralized_nlp_package.nli_utils.run_glue"], [6, "module-centralized_nlp_package.text_processing.text_analysis"], [6, "module-centralized_nlp_package.text_processing.text_preprocessing"], [6, "module-centralized_nlp_package.text_processing.text_utils"], [7, "module-centralized_nlp_package.utils.helper"]], "query_constructor() (in module centralized_nlp_package.common_utils.string_utils)": [[1, "centralized_nlp_package.common_utils.string_utils.query_constructor"]], "centralized_nlp_package.data_access.snowflake_utils": [[2, "module-centralized_nlp_package.data_access.snowflake_utils"]], "execute_truncate_or_merge_query() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.execute_truncate_or_merge_query"]], "get_snowflake_connection_options() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.get_snowflake_connection_options"]], "read_from_snowflake() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.read_from_snowflake"]], "retrieve_snowflake_private_key() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.retrieve_snowflake_private_key"]], "singleton() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.singleton"]], "with_spark_session() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.with_spark_session"]], "write_dataframe_to_snowflake() (in module centralized_nlp_package.data_access.snowflake_utils)": [[2, "centralized_nlp_package.data_access.snowflake_utils.write_dataframe_to_snowflake"]], "centralized_nlp_package.data_processing.dask_utils": [[3, "module-centralized_nlp_package.data_processing.dask_utils"]], "centralized_nlp_package.data_processing.dataframe_utils": [[3, "module-centralized_nlp_package.data_processing.dataframe_utils"]], "centralized_nlp_package.data_processing.spark_utils": [[3, "module-centralized_nlp_package.data_processing.spark_utils"]], "check_pd_dataframe_for_records() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.check_pd_dataframe_for_records"]], "check_spark_dataframe_for_records() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.check_spark_dataframe_for_records"]], "concatenate_and_reset_index() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.concatenate_and_reset_index"]], "convert_columns_to_timestamp() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.convert_columns_to_timestamp"]], "create_spark_udf() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.create_spark_udf"]], "dask_compute_with_progress() (in module centralized_nlp_package.data_processing.dask_utils)": [[3, "centralized_nlp_package.data_processing.dask_utils.dask_compute_with_progress"]], "define_structure() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.define_structure"]], "df_apply_transformations() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.df_apply_transformations"]], "df_remove_rows_with_keywords() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.df_remove_rows_with_keywords"]], "equivalent_type() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.equivalent_type"]], "get_default_dtype_mapping() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.get_default_dtype_mapping"]], "initialize_dask_client() (in module centralized_nlp_package.data_processing.dask_utils)": [[3, "centralized_nlp_package.data_processing.dask_utils.initialize_dask_client"]], "initialize_spark_session() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.initialize_spark_session"]], "keyword_to_datatype() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.keyword_to_datatype"]], "pandas_to_spark() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.pandas_to_spark"]], "save_report() (in module centralized_nlp_package.data_processing.dataframe_utils)": [[3, "centralized_nlp_package.data_processing.dataframe_utils.save_report"]], "sparkdf_apply_transformations() (in module centralized_nlp_package.data_processing.spark_utils)": [[3, "centralized_nlp_package.data_processing.spark_utils.sparkdf_apply_transformations"]], "average_token_embeddings() (in module centralized_nlp_package.embedding.embedding_utils)": [[4, "centralized_nlp_package.embedding.embedding_utils.average_token_embeddings"]], "centralized_nlp_package.embedding.embedding_utils": [[4, "module-centralized_nlp_package.embedding.embedding_utils"]], "centralized_nlp_package.embedding.word2vec_model": [[4, "module-centralized_nlp_package.embedding.word2vec_model"]], "save_word2vec_model() (in module centralized_nlp_package.embedding.word2vec_model)": [[4, "centralized_nlp_package.embedding.word2vec_model.save_word2vec_model"]], "train_word2vec_model() (in module centralized_nlp_package.embedding.word2vec_model)": [[4, "centralized_nlp_package.embedding.word2vec_model.train_word2vec_model"]], "datatrainingarguments (class in centralized_nlp_package.nli_utils.arguments)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments"]], "modelarguments (class in centralized_nlp_package.nli_utils.arguments)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments"]], "cache_dir (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.cache_dir"], [5, "id12"]], "centralized_nlp_package.nli_utils.arguments": [[5, "module-centralized_nlp_package.nli_utils.arguments"]], "centralized_nlp_package.nli_utils.data": [[5, "module-centralized_nlp_package.nli_utils.data"]], "centralized_nlp_package.nli_utils.metrics": [[5, "module-centralized_nlp_package.nli_utils.metrics"]], "centralized_nlp_package.nli_utils.nli_inference": [[5, "module-centralized_nlp_package.nli_utils.nli_inference"]], "centralized_nlp_package.nli_utils.nli_trainer": [[5, "module-centralized_nlp_package.nli_utils.nli_trainer"]], "centralized_nlp_package.nli_utils.run_glue": [[5, "module-centralized_nlp_package.nli_utils.run_glue"]], "config_name (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.config_name"]], "dataset_config_name (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.dataset_config_name"], [5, "id0"]], "dataset_name (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.dataset_name"], [5, "id1"]], "evaluate() (in module centralized_nlp_package.nli_utils.nli_trainer)": [[5, "centralized_nlp_package.nli_utils.nli_trainer.evaluate"]], "get_compute_metrics() (in module centralized_nlp_package.nli_utils.metrics)": [[5, "centralized_nlp_package.nli_utils.metrics.get_compute_metrics"]], "ignore_mismatched_sizes (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.ignore_mismatched_sizes"]], "initialize_nli_infer_pipeline() (in module centralized_nlp_package.nli_utils.nli_inference)": [[5, "centralized_nlp_package.nli_utils.nli_inference.initialize_nli_infer_pipeline"]], "initialize_trainer() (in module centralized_nlp_package.nli_utils.nli_trainer)": [[5, "centralized_nlp_package.nli_utils.nli_trainer.initialize_trainer"]], "learning_rate (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.learning_rate"], [5, "id13"]], "main() (in module centralized_nlp_package.nli_utils.run_glue)": [[5, "centralized_nlp_package.nli_utils.run_glue.main"]], "max_eval_samples (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.max_eval_samples"], [5, "id2"]], "max_predict_samples (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.max_predict_samples"], [5, "id3"]], "max_seq_length (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.max_seq_length"], [5, "id4"]], "max_train_samples (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.max_train_samples"], [5, "id5"]], "model_name_or_path (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.model_name_or_path"], [5, "id14"]], "model_revision (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.model_revision"]], "overwrite_cache (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.overwrite_cache"], [5, "id6"]], "pad_to_max_length (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.pad_to_max_length"], [5, "id7"]], "per_device_eval_batch_size (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.per_device_eval_batch_size"], [5, "id15"]], "per_device_train_batch_size (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.per_device_train_batch_size"], [5, "id16"]], "predict() (in module centralized_nlp_package.nli_utils.nli_trainer)": [[5, "centralized_nlp_package.nli_utils.nli_trainer.predict"]], "prepare_datasets() (in module centralized_nlp_package.nli_utils.data)": [[5, "centralized_nlp_package.nli_utils.data.prepare_datasets"]], "preprocess_datasets() (in module centralized_nlp_package.nli_utils.data)": [[5, "centralized_nlp_package.nli_utils.data.preprocess_datasets"]], "run_glue() (in module centralized_nlp_package.nli_utils.run_glue)": [[5, "centralized_nlp_package.nli_utils.run_glue.run_glue"]], "setup_logging() (in module centralized_nlp_package.nli_utils.nli_trainer)": [[5, "centralized_nlp_package.nli_utils.nli_trainer.setup_logging"]], "task_name (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.task_name"], [5, "id8"]], "test_file (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.test_file"], [5, "id9"]], "token (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.token"]], "tokenizer_name (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.tokenizer_name"]], "train() (in module centralized_nlp_package.nli_utils.nli_trainer)": [[5, "centralized_nlp_package.nli_utils.nli_trainer.train"]], "train_file (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.train_file"], [5, "id10"]], "trust_remote_code (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.trust_remote_code"]], "use_fast_tokenizer (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.use_fast_tokenizer"]], "validation_file (centralized_nlp_package.nli_utils.arguments.datatrainingarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.DataTrainingArguments.validation_file"], [5, "id11"]], "weight_decay (centralized_nlp_package.nli_utils.arguments.modelarguments attribute)": [[5, "centralized_nlp_package.nli_utils.arguments.ModelArguments.weight_decay"], [5, "id17"]], "calculate_polarity_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.calculate_polarity_score"]], "calculate_sentence_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.calculate_sentence_score"]], "centralized_nlp_package.text_processing.text_analysis": [[6, "module-centralized_nlp_package.text_processing.text_analysis"]], "centralized_nlp_package.text_processing.text_preprocessing": [[6, "module-centralized_nlp_package.text_processing.text_preprocessing"]], "centralized_nlp_package.text_processing.text_utils": [[6, "module-centralized_nlp_package.text_processing.text_utils"]], "check_negation() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.check_negation"]], "clean_text() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[6, "centralized_nlp_package.text_processing.text_preprocessing.clean_text"]], "combine_sentiment_scores() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.combine_sentiment_scores"]], "expand_contractions() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.expand_contractions"]], "fog_analysis_per_section() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.fog_analysis_per_section"]], "fog_analysis_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.fog_analysis_per_sentence"]], "generate_match_count() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.generate_match_count"]], "generate_ngrams() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.generate_ngrams"]], "generate_sentence_relevance_score() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.generate_sentence_relevance_score"]], "generate_topic_statistics() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.generate_topic_statistics"]], "get_match_set() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.get_match_set"]], "initialize_spacy() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[6, "centralized_nlp_package.text_processing.text_preprocessing.initialize_spacy"]], "is_complex() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.is_complex"]], "load_set_from_txt() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.load_set_from_txt"]], "load_syllable_counts() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.load_syllable_counts"]], "load_word_set() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.load_word_set"]], "match_count() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.match_count"]], "merge_counts() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.merge_counts"]], "netscore() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.netscore"]], "polarity_score_per_section() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.polarity_score_per_section"]], "polarity_score_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.polarity_score_per_sentence"]], "preprocess_text() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[6, "centralized_nlp_package.text_processing.text_preprocessing.preprocess_text"]], "preprocess_text_list() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[6, "centralized_nlp_package.text_processing.text_preprocessing.preprocess_text_list"]], "remove_unwanted_phrases_and_validate() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[6, "centralized_nlp_package.text_processing.text_preprocessing.remove_unwanted_phrases_and_validate"]], "tokenize_and_lemmatize_text() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[6, "centralized_nlp_package.text_processing.text_preprocessing.tokenize_and_lemmatize_text"]], "tokenize_matched_words() (in module centralized_nlp_package.text_processing.text_preprocessing)": [[6, "centralized_nlp_package.text_processing.text_preprocessing.tokenize_matched_words"]], "tokenize_text() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.tokenize_text"]], "tone_count_with_negation_check() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.tone_count_with_negation_check"]], "tone_count_with_negation_check_per_sentence() (in module centralized_nlp_package.text_processing.text_analysis)": [[6, "centralized_nlp_package.text_processing.text_analysis.tone_count_with_negation_check_per_sentence"]], "validate_and_format_text() (in module centralized_nlp_package.text_processing.text_utils)": [[6, "centralized_nlp_package.text_processing.text_utils.validate_and_format_text"]], "centralized_nlp_package.utils.helper": [[7, "module-centralized_nlp_package.utils.helper"]], "determine_environment() (in module centralized_nlp_package.utils.helper)": [[7, "centralized_nlp_package.utils.helper.determine_environment"]], "load_config_from_file() (in module centralized_nlp_package.utils.helper)": [[7, "centralized_nlp_package.utils.helper.load_config_from_file"]]}})